{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fb0db1-983f-4d51-9a5b-c98812d6dbce",
   "metadata": {},
   "source": [
    "# Afterpulse Detection (dead-time / internal reflections)\n",
    "To figure out how to detect and remove afterpulses, and plot examples.\n",
    "- use photon rate just around the lake surface elevation (depending on strong/weak beam??)\n",
    "- then use peaks distances that are commonly found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7fafd-8cb8-4060-8fd7-473d239398a6",
   "metadata": {},
   "source": [
    "### From the literature\n",
    "afterpulses at: ∼0.45, ∼0.9, ∼2.3, and ∼4.2 m\n",
    "\n",
    "The afterpulses captured from on-orbit measurements are caused by three different reasons: (1) the effects of the dead-time circuit (∼3 ns) due to PMT saturation; (2) the effects of optical reflections within the ATLAS receiver optical components; (3) PMT afterpulses. The echoes separated by ∼0.45 m are attributed to the effect of the dead-time circuit (∼3 ns) due to PMT saturation. The echoes at ∼2.3 and ∼4.2 m below the primary surface returns are caused by the optical reflections within the ATLAS receiver optical components, while the echoes from ∼10 to ∼45 m away from the primary surface signal are due to the PMT afterpulses with a longer time delay.\n",
    "\n",
    "Lu, X., Hu, Y., Yang, Y., Vaughan, M., Palm, S., Trepte, C., ... & Baize, R. (2021). Enabling value added scientific applications of ICESat‐2 data with effective removal of afterpulses. Earth and Space Science, 8(6), e2021EA001729."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b197d712-bc64-4ce0-9e4a-6238f7c627b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import Image, display\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.signal import find_peaks\n",
    "import hdbscan\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f05c40f-a0a9-4363-86b6-d4dcfa20fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_string(dt):\n",
    "    epoch = dt + datetime.datetime.timestamp(datetime.datetime(2018,1,1))\n",
    "    return datetime.datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "\n",
    "def read_melt_lake_h5(fn):\n",
    "    \n",
    "    lakedict = {}\n",
    "    with h5py.File(fn, 'r') as f:\n",
    "\n",
    "        # metadata\n",
    "        for key in f['properties'].keys(): \n",
    "            lakedict[key] = f['properties'][key][()]\n",
    "            if f['properties'][key].dtype == object:\n",
    "                lakedict[key] = lakedict[key].decode('utf-8')\n",
    "                \n",
    "\n",
    "        # photon data\n",
    "        photon_data_dict = {}\n",
    "        for key in f['photon_data'].keys():\n",
    "            photon_data_dict[key] = f['photon_data'][key][()]\n",
    "        lakedict['photon_data'] = pd.DataFrame(photon_data_dict)\n",
    "\n",
    "        # mframe data\n",
    "        mframe_data_dict = {}\n",
    "        for key in f['mframe_data'].keys():\n",
    "            mframe_data_dict[key] = f['mframe_data'][key][()]\n",
    "        lakedict['mframe_data'] = pd.DataFrame(mframe_data_dict).set_index('mframe')    \n",
    "\n",
    "        # second returns data\n",
    "        det_2nds_dict = {}\n",
    "        for key in f['detection_2nd_returns'].keys():\n",
    "            det_2nds_dict[key] = f['detection_2nd_returns'][key][()]\n",
    "        lakedict['detection_2nd_returns'] = det_2nds_dict\n",
    "\n",
    "        # quality assessment data\n",
    "        qual_dict = {}\n",
    "        for key in f['detection_quality_info'].keys():\n",
    "            qual_dict[key] = f['detection_quality_info'][key][()]\n",
    "        lakedict['detection_quality_info'] = qual_dict\n",
    "        \n",
    "        # re-nest the lake extent segments\n",
    "        def re_nest_extent(x): return [[x[i], x[i+1]] for i in np.arange(0,len(x),2)]\n",
    "        lakedict['surface_extent_detection'] = re_nest_extent(lakedict['surface_extent_detection'])\n",
    "        lakedict['lat_surface_extent_detection'] = re_nest_extent(lakedict['lat_surface_extent_detection'])\n",
    "        \n",
    "        lakedict['date_time'] = convert_time_to_string(lakedict['mframe_data']['dt'].mean())\n",
    "        \n",
    "        return lakedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74b276b-647d-4648-affb-d42a035e7405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchfor = 'lake_'\n",
    "searchdir = 'WAIS_region_comps/'\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f) & ('.h5' in f)]\n",
    "\n",
    "searchfor = 'lake_'\n",
    "searchdir = 'WAIS_2018-22-meltregions/'\n",
    "filelist += [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f) & ('.h5' in f) & ('george' not in f)]\n",
    "\n",
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33f9442-d993-4332-8e30-29100a9a29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_upper = 1.005\n",
    "thresh_lower = -5.505\n",
    "bin_h=0.01\n",
    "smooth_h=0.1\n",
    "extent_buffer = 21.0\n",
    "smooth_pulse = 15 # makes it roughly the footprint size\n",
    "strength = 'all'\n",
    "saturation_threshold = 0.99\n",
    "\n",
    "cols_pk = ['black','#CD104D', '#E14D2A', '#FD841F', '#8FE3CF', '#256D85']\n",
    "lsty_pk = ['-', '-', '--', ':', '-', ':']\n",
    "elev_pk_lake = [0.0, -0.55, -0.845, -1.36, -2.33, -4.17]\n",
    "elev_pk_puls = [0.0, -0.55, -0.91, -1.465, -2.33, -4.17]\n",
    "widths_pk = [0.225, 0.225, 0.225, 0.225, 0.3, 0.3] # 0.225 m on each side makes it 0.45 total, which is the dead-time for ATLAS\n",
    "df_pks_info = pd.DataFrame({'h_lake': elev_pk_lake, 'h_pulse': elev_pk_puls, 'width': widths_pk, 'color': cols_pk, 'ls': lsty_pk})\n",
    "\n",
    "def group_by_pulse(df_in, smoothing, beam_strength):\n",
    "    thegroup = df_in.groupby('pulseid')\n",
    "    df_grouped = thegroup[['xatc', 'h']].mean()\n",
    "    norm_factor = 4 if beam_strength == 'weak' else 16\n",
    "    df_grouped['ph_count'] = thegroup['h'].count() / norm_factor\n",
    "    df_grouped['ph_count_smooth'] = np.array(df_grouped.ph_count.rolling(smoothing,center=True,min_periods=1).mean())\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94ec12ef-4587-4d82-888f-5302af689692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>h</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>mframe</th>\n",
       "      <th>ph_id_pulse</th>\n",
       "      <th>snr</th>\n",
       "      <th>xatc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-37.086669</td>\n",
       "      <td>441.577514</td>\n",
       "      <td>-85.425326</td>\n",
       "      <td>-146.035034</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-37.086669</td>\n",
       "      <td>436.644682</td>\n",
       "      <td>-85.425326</td>\n",
       "      <td>-146.035032</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057995</td>\n",
       "      <td>0.012732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.825213</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.289988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.775073</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956354</td>\n",
       "      <td>0.290397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.740954</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968186</td>\n",
       "      <td>0.290397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27512</th>\n",
       "      <td>-37.032911</td>\n",
       "      <td>234.355710</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096788</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.314809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27513</th>\n",
       "      <td>-37.032911</td>\n",
       "      <td>233.478803</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096788</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.317266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27514</th>\n",
       "      <td>-37.032892</td>\n",
       "      <td>340.949457</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096880</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.746818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27515</th>\n",
       "      <td>-37.032887</td>\n",
       "      <td>298.023243</td>\n",
       "      <td>-85.435572</td>\n",
       "      <td>-146.096856</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.858222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27516</th>\n",
       "      <td>-37.032881</td>\n",
       "      <td>248.738020</td>\n",
       "      <td>-85.435575</td>\n",
       "      <td>-146.096830</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.986439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27517 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid           h        lat         lon      mframe  ph_id_pulse  \\\n",
       "0     -37.086669  441.577514 -85.425326 -146.035034  4031877160            1   \n",
       "1     -37.086669  436.644682 -85.425326 -146.035032  4031877160            1   \n",
       "2     -37.086657  329.825213 -85.425331 -146.034974  4031877160            1   \n",
       "3     -37.086657  329.775073 -85.425331 -146.034974  4031877160            1   \n",
       "4     -37.086657  329.740954 -85.425331 -146.034974  4031877160            1   \n",
       "...          ...         ...        ...         ...         ...          ...   \n",
       "27512 -37.032911  234.355710 -85.435570 -146.096788  4031877168          199   \n",
       "27513 -37.032911  233.478803 -85.435570 -146.096788  4031877168          199   \n",
       "27514 -37.032892  340.949457 -85.435570 -146.096880  4031877168          200   \n",
       "27515 -37.032887  298.023243 -85.435572 -146.096856  4031877168          200   \n",
       "27516 -37.032881  248.738020 -85.435575 -146.096830  4031877168          200   \n",
       "\n",
       "            snr         xatc  \n",
       "0      0.000000     0.000000  \n",
       "1      0.057995     0.012732  \n",
       "2      0.925701     0.289988  \n",
       "3      0.956354     0.290397  \n",
       "4      0.968186     0.290397  \n",
       "...         ...          ...  \n",
       "27512  0.000000  1269.314809  \n",
       "27513  0.000000  1269.317266  \n",
       "27514  0.000000  1269.746818  \n",
       "27515  0.000000  1269.858222  \n",
       "27516  0.000000  1269.986439  \n",
       "\n",
       "[27517 rows x 8 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b11519bb-b9ab-4655-8da0-c2e0e1cef692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125 3141 16\n",
      "0.003323793159282218 2.2173961156035604e-11\n",
      "134.36367904834333\n",
      "5.186527713857285\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "hvals = np.random.normal(5,1,5545)\n",
    "num_channels = 16\n",
    "dead_time = 2.979375e-09\n",
    "\n",
    "speed_of_light = 299792458 #m/s\n",
    "hvals = np.sort(hvals)\n",
    "diffs = np.abs(hvals[num_channels-1:] - hvals[:-num_channels+1])\n",
    "start_saturated = np.argmin(diffs)\n",
    "end_saturated = start_saturated+num_channels\n",
    "hvals_saturated = hvals[start_saturated:end_saturated]\n",
    "hdiff_saturated = np.min(diffs)\n",
    "timediff_saturated = 2 * hdiff_saturated / speed_of_light\n",
    "percent_of_saturated = dead_time/timediff_saturated\n",
    "elev_saturated = np.median(hvals_saturated)\n",
    "print(start_saturated, end_saturated, len(hvals_saturated))\n",
    "print(hdiff_saturated, timediff_saturated)\n",
    "print(percent_of_saturated)\n",
    "print(elev_saturated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db6f4ea8-8853-4507-98a6-ce35d670a389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>h</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>mframe</th>\n",
       "      <th>ph_id_pulse</th>\n",
       "      <th>snr</th>\n",
       "      <th>xatc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-37.086669</td>\n",
       "      <td>441.577514</td>\n",
       "      <td>-85.425326</td>\n",
       "      <td>-146.035034</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-37.086669</td>\n",
       "      <td>436.644682</td>\n",
       "      <td>-85.425326</td>\n",
       "      <td>-146.035032</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057995</td>\n",
       "      <td>0.012732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.825213</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.289988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.775073</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956354</td>\n",
       "      <td>0.290397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37.086657</td>\n",
       "      <td>329.740954</td>\n",
       "      <td>-85.425331</td>\n",
       "      <td>-146.034974</td>\n",
       "      <td>4031877160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968186</td>\n",
       "      <td>0.290397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27512</th>\n",
       "      <td>-37.032911</td>\n",
       "      <td>234.355710</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096788</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.314809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27513</th>\n",
       "      <td>-37.032911</td>\n",
       "      <td>233.478803</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096788</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.317266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27514</th>\n",
       "      <td>-37.032892</td>\n",
       "      <td>340.949457</td>\n",
       "      <td>-85.435570</td>\n",
       "      <td>-146.096880</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.746818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27515</th>\n",
       "      <td>-37.032887</td>\n",
       "      <td>298.023243</td>\n",
       "      <td>-85.435572</td>\n",
       "      <td>-146.096856</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.858222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27516</th>\n",
       "      <td>-37.032881</td>\n",
       "      <td>248.738020</td>\n",
       "      <td>-85.435575</td>\n",
       "      <td>-146.096830</td>\n",
       "      <td>4031877168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1269.986439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27517 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid           h        lat         lon      mframe  ph_id_pulse  \\\n",
       "0     -37.086669  441.577514 -85.425326 -146.035034  4031877160            1   \n",
       "1     -37.086669  436.644682 -85.425326 -146.035032  4031877160            1   \n",
       "2     -37.086657  329.825213 -85.425331 -146.034974  4031877160            1   \n",
       "3     -37.086657  329.775073 -85.425331 -146.034974  4031877160            1   \n",
       "4     -37.086657  329.740954 -85.425331 -146.034974  4031877160            1   \n",
       "...          ...         ...        ...         ...         ...          ...   \n",
       "27512 -37.032911  234.355710 -85.435570 -146.096788  4031877168          199   \n",
       "27513 -37.032911  233.478803 -85.435570 -146.096788  4031877168          199   \n",
       "27514 -37.032892  340.949457 -85.435570 -146.096880  4031877168          200   \n",
       "27515 -37.032887  298.023243 -85.435572 -146.096856  4031877168          200   \n",
       "27516 -37.032881  248.738020 -85.435575 -146.096830  4031877168          200   \n",
       "\n",
       "            snr         xatc  \n",
       "0      0.000000     0.000000  \n",
       "1      0.057995     0.012732  \n",
       "2      0.925701     0.289988  \n",
       "3      0.956354     0.290397  \n",
       "4      0.968186     0.290397  \n",
       "...         ...          ...  \n",
       "27512  0.000000  1269.314809  \n",
       "27513  0.000000  1269.317266  \n",
       "27514  0.000000  1269.746818  \n",
       "27515  0.000000  1269.858222  \n",
       "27516  0.000000  1269.986439  \n",
       "\n",
       "[27517 rows x 8 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9dd2e4f8-e06e-4cea-8b4e-89f0bba27df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36 49 64 81]\n",
      "[ 1  3  5  7  9 11 13 15 17]\n",
      "7 3 9\n"
     ]
    }
   ],
   "source": [
    "hs = np.arange(10)**2\n",
    "diffs = np.diff(hs)\n",
    "first_saturated = next((x for x in diffs if x >= 6), None)\n",
    "first_saturated_idx = next(i for i,val in enumerate(diffs) if val >= 6)\n",
    "print(hs)\n",
    "print(diffs)\n",
    "print(first_saturated, first_saturated_idx, hs[first_saturated_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ef52a3f4-d519-4727-997c-8040ff47c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function for estimating ratio saturated and saturated peak better\n",
    "def get_saturation_and_elevation(hvals, num_channels, dead_time):\n",
    "    speed_of_light = 299792458 #m/s\n",
    "    hvals = np.flip(np.sort(hvals))\n",
    "    if len(hvals) < num_channels:\n",
    "        return pd.Series({'frac_saturation': 0.0, 'elev_saturation': np.nan})\n",
    "    else:\n",
    "        diffs = np.abs(hvals[num_channels-1:] - hvals[:-num_channels+1])\n",
    "        diff_threshold_saturated = dead_time * speed_of_light / 2.0\n",
    "        diff_means_saturated = diffs <= diff_threshold_saturated\n",
    "        if np.sum(diff_means_saturated) > 1:\n",
    "            first_saturated_idx = next(i for i,val in enumerate(diffs) if val <= diff_threshold_saturated)\n",
    "            first_saturated_elev = hvals[first_saturated_idx+num_channels-1]\n",
    "            hvals = hvals[hvals > first_saturated_elev- diff_threshold_saturated/2]\n",
    "            diffs = np.abs(hvals[num_channels-1:] - hvals[:-num_channels+1])\n",
    "        start_saturated = np.argmin(diffs)\n",
    "        end_saturated = start_saturated+num_channels\n",
    "        hvals_saturated = hvals[start_saturated:end_saturated]\n",
    "        hdiff_saturated = np.min(diffs)\n",
    "        timediff_saturated = 2 * hdiff_saturated / speed_of_light\n",
    "        fraction_of_saturated = dead_time/timediff_saturated\n",
    "        elev_saturated = np.median(hvals_saturated)\n",
    "        return pd.Series({'frac_saturation': fraction_of_saturated, 'elev_saturation': elev_saturated})\n",
    "        \n",
    "# def group_by_pulse(df_in, beam_strength, dead_time):\n",
    "#     thegroup = df_in.groupby('pulseid')\n",
    "#     df_grouped = thegroup[['xatc', 'lat', 'lon']].mean()\n",
    "#     n_channels = 4 if beam_strength == 'weak' else 16\n",
    "\n",
    "#     df_grouped['ph_count'] = thegroup['h'].count() / n_channels\n",
    "#     return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dbe3e1c0-6fb0-4779-ab1f-67758646aec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xatc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>frac_saturation</th>\n",
       "      <th>elev_saturation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulseid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1573424383001</th>\n",
       "      <td>0.100872</td>\n",
       "      <td>67.271202</td>\n",
       "      <td>-49.004869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424383002</th>\n",
       "      <td>0.826373</td>\n",
       "      <td>67.271208</td>\n",
       "      <td>-49.004871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424383003</th>\n",
       "      <td>1.452891</td>\n",
       "      <td>67.271214</td>\n",
       "      <td>-49.004873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424383004</th>\n",
       "      <td>2.211842</td>\n",
       "      <td>67.271220</td>\n",
       "      <td>-49.004875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424383005</th>\n",
       "      <td>2.889557</td>\n",
       "      <td>67.271226</td>\n",
       "      <td>-49.004877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424401196</th>\n",
       "      <td>2695.158131</td>\n",
       "      <td>67.295206</td>\n",
       "      <td>-49.012075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424401197</th>\n",
       "      <td>2695.927369</td>\n",
       "      <td>67.295213</td>\n",
       "      <td>-49.012077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424401198</th>\n",
       "      <td>2696.632737</td>\n",
       "      <td>67.295219</td>\n",
       "      <td>-49.012079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424401199</th>\n",
       "      <td>2697.300512</td>\n",
       "      <td>67.295225</td>\n",
       "      <td>-49.012081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573424401200</th>\n",
       "      <td>2698.043676</td>\n",
       "      <td>67.295231</td>\n",
       "      <td>-49.012083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3797 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      xatc        lat        lon  frac_saturation  \\\n",
       "pulseid                                                             \n",
       "1573424383001     0.100872  67.271202 -49.004869              0.0   \n",
       "1573424383002     0.826373  67.271208 -49.004871              0.0   \n",
       "1573424383003     1.452891  67.271214 -49.004873              0.0   \n",
       "1573424383004     2.211842  67.271220 -49.004875              0.0   \n",
       "1573424383005     2.889557  67.271226 -49.004877              0.0   \n",
       "...                    ...        ...        ...              ...   \n",
       "1573424401196  2695.158131  67.295206 -49.012075              0.0   \n",
       "1573424401197  2695.927369  67.295213 -49.012077              0.0   \n",
       "1573424401198  2696.632737  67.295219 -49.012079              0.0   \n",
       "1573424401199  2697.300512  67.295225 -49.012081              0.0   \n",
       "1573424401200  2698.043676  67.295231 -49.012083              0.0   \n",
       "\n",
       "               elev_saturation  \n",
       "pulseid                         \n",
       "1573424383001              NaN  \n",
       "1573424383002              NaN  \n",
       "1573424383003              NaN  \n",
       "1573424383004              NaN  \n",
       "1573424383005              NaN  \n",
       "...                        ...  \n",
       "1573424401196              NaN  \n",
       "1573424401197              NaN  \n",
       "1573424401198              NaN  \n",
       "1573424401199              NaN  \n",
       "1573424401200              NaN  \n",
       "\n",
       "[3797 rows x 5 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = 'WAIS_2018-22-meltregions/lake_09475_AIS_2021-22_waismeltregions_nimrod_ATL03_20211221220536_13811311_005_01_gt2r.h5'\n",
    "# filename = 'WAIS_2018-22-meltregions/lake_10000_AIS_2021-22_waismeltregions_scott_ATL03_20211218201400_13341311_005_01_gt2l.h5'\n",
    "filename = 'WAIS_region_comps/lake_07805_GrIS_2020_west_greenland_ATL03_20200713115804_02770803_005_01_gt2l.h5'\n",
    "filename = 'WAIS_region_comps/lake_05676_GrIS_2021_west_greenland_ATL03_20210715182907_03381203_005_01_gt2r.h5'\n",
    "\n",
    "lk = read_melt_lake_h5(filename)\n",
    "df = lk['photon_data']\n",
    "beam_strength = lk['beam_strength']\n",
    "deadtime = lk['dead_time']\n",
    "n_channels = 4 if beam_strength == 'weak' else 16\n",
    "df['pulseid'] = 1000*df.mframe.astype(np.uint64)+df.ph_id_pulse.astype(np.uint64)\n",
    "df['ph_index'] = df.index\n",
    "df = df.set_index('pulseid')\n",
    "thegroup = df.groupby('pulseid')\n",
    "df_grouped = thegroup[['xatc', 'lat', 'lon']].mean()\n",
    "kwargs = {'num_channels': n_channels, 'dead_time': deadtime}\n",
    "saturation_fraction_and_elevation = thegroup[['h']].apply(get_saturation_and_elevation, **kwargs)\n",
    "df_grouped = pd.concat([df_grouped, saturation_fraction_and_elevation], axis=1)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "24bf16fb-272b-4ab3-97eb-dbf4473709f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdb822c1b2748908c43690a776c6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grouped['xatc_adjust'] = np.nan\n",
    "is_saturated = df_grouped.frac_saturation > 1.0\n",
    "df_grouped.loc[is_saturated, 'xatc_adjust'] = [0.7 * i for i in range(np.sum(is_saturated))]\n",
    "df_merged = df.join(df_grouped[['frac_saturation', 'elev_saturation', 'xatc_adjust']], on='pulseid', how='left')\n",
    "df_merged['h_adjust'] = df_merged.h - df_merged.elev_saturation\n",
    "\n",
    "fig, axs = plt.subplots(figsize=[8,5], dpi=100, nrows=2)\n",
    "ax = axs[0]\n",
    "# ax.scatter(df.xatc, df.h, s=2, color='k', edgecolors='none')\n",
    "ax.scatter(df_merged.xatc, df_merged.h, s=4, \n",
    "           c=df_merged.frac_saturation, alpha=1, edgecolors='none', cmap=cmc.batlow, vmin=0, vmax=4)\n",
    "ax.scatter(df_grouped.xatc[is_saturated], df_grouped.elev_saturation[is_saturated], s=5, color='c')\n",
    "# ax.scatter(df_merged.xatc[is_saturated], df_merged.h[is_saturated], s=4, \n",
    "#            c='k', alpha=1, edgecolors='none', cmap=cmc.lajolla, vmin=1, vmax=5)\n",
    "ax.set_ylim((1197.4848135682196, 1212.9346739038342))\n",
    "ax.set_xlim((233.87586705780086, 2446.7673782822703))\n",
    "\n",
    "ax = axs[1]\n",
    "is_saturated = df_merged.frac_saturation > 1.0\n",
    "ax.scatter(df_merged.xatc_adjust[is_saturated], df_merged.h_adjust[is_saturated], s=2, color='k', alpha=1, edgecolors='none')\n",
    "ax.set_ylim((1197.4848135682196, 1212.9346739038342)-df_merged.elev_saturation.median())\n",
    "# ax.set_xlim((233.87586705780086, 2446.7673782822703))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "58e83832-0604-4507-a68f-e5765874a991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fae416c913d455bace7ebcbdf73160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,5], dpi=100)\n",
    "ax.hist(df_grouped.frac_saturation[df_grouped.frac_saturation>=1.0], bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01066cf-99c3-47a5-a137-ecac38f71cb8",
   "metadata": {},
   "source": [
    "## read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d7c9dd-c83d-4ccf-82d8-e262d8da274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!ng in lake files: 2940 / 2940\n"
     ]
    }
   ],
   "source": [
    "# filelist = filelist[:10]\n",
    "df_list = []\n",
    "perc_sat_list = []\n",
    "n_lakes = 0\n",
    "for i,filename in enumerate(filelist):\n",
    "    \n",
    "    lk = read_melt_lake_h5(filename)\n",
    "    \n",
    "    surf_elev = lk['surface_elevation']\n",
    "    df = lk['photon_data']\n",
    "    beam_strength = lk['beam_strength']\n",
    "    dfs = df[(df.h < (thresh_upper+surf_elev)) & (df.h > (thresh_lower+surf_elev))].copy()\n",
    "    dfs['h'] = dfs.h - surf_elev\n",
    "    dfs['pulseid'] = dfs.apply(lambda row: 1000*row.mframe+row.ph_id_pulse, axis=1)\n",
    "    dfs['ph_index'] = dfs.index\n",
    "    dfs = dfs.set_index('pulseid')\n",
    "    pkinfo = df_pks_info.iloc[0]\n",
    "    photon_df = dfs[(dfs.h >= (pkinfo.h_pulse - 0.2)) & (dfs.h < (pkinfo.h_pulse + 0.3))]\n",
    "    df_pulses = group_by_pulse(photon_df,smooth_pulse,beam_strength)\n",
    "    df_join = dfs.join(df_pulses, how='left', rsuffix='_pulse')\n",
    "    df_join['pulseid'] = dfs.index\n",
    "    df_join = df_join.set_index('ph_index')\n",
    "    df_join['h_relative_to_saturated_peak'] = df_join.h - df_join.h_pulse\n",
    "    df_join['is_saturated'] = df_join.ph_count_smooth >= saturation_threshold\n",
    "    df_join['beam_strength'] = beam_strength\n",
    "    df_join['beam_number'] = lk['beam_number']\n",
    "    df_join['gtx'] = lk['gtx']\n",
    "    df_join['lake_nr'] = i\n",
    "    \n",
    "    perc_sat_list.append(np.sum(df_join['is_saturated']) / len(df_join))\n",
    "    \n",
    "    df_list.append(df_join)\n",
    "    \n",
    "    print('reading in lake files: %4i / %4i' % (i+1,len(filelist)), end='\\r')\n",
    "    \n",
    "dfs_all = pd.concat(df_list)\n",
    "del df_list\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8aefdae-1e04-4e7d-a895-b8f128646705",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_select = 'all'\n",
    "if type(beam_select) == int:\n",
    "    dfs = dfs_all[dfs_all.beam_number == beam_select]\n",
    "    thisstrength = 'strong' if beam_select%2==1 else 'weak'\n",
    "    beam_select = '#%i, %s' % (beam_select, thisstrength)\n",
    "elif 'gt' in beam_select:\n",
    "    dfs = dfs_all[dfs_all.gtx == beam_select]\n",
    "elif beam_select == 'all':\n",
    "    dfs = dfs_all\n",
    "else:\n",
    "    dfs = dfs_all[dfs_all.beam_strength == beam_select]\n",
    "df_saturated = dfs[dfs.ph_count > 0.999]\n",
    "n_lakes = len(np.unique(dfs.lake_nr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bccc8a4-b5cd-43a6-a97d-824fdc78a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0.89% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_09760_AIS_2021-22_waismeltregions_nimrod_ATL03_20211221220536_13811311_005_01_gt2l.h5'\n",
      "# 0.86% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_10000_AIS_2021-22_waismeltregions_scott_ATL03_20211218201400_13341311_005_01_gt2l.h5'\n",
      "# 0.82% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_region_comps/lake_09165_GrIS_2020_west_greenland_ATL03_20200713115804_02770803_005_01_gt2l.h5'\n",
      "# 0.79% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_region_comps/lake_09700_AIS_2019-20_amery_ATL03_20200118002319_03400610_005_01_gt2r.h5'\n",
      "# 0.79% saturated, beam 4 (weak)\n",
      "#fn = 'WAIS_region_comps/lake_09206_AIS_2019-20_amery_ATL03_20200118002319_03400610_005_01_gt2l.h5'\n",
      "# 0.79% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_09865_AIS_2021-22_waismeltregions_larsen_ATL03_20220116110420_03841412_005_01_gt2l.h5'\n",
      "# 0.76% saturated, beam 4 (weak)\n",
      "#fn = 'WAIS_region_comps/lake_10000_GrIS_2019_west_greenland_ATL03_20190613193602_11690305_005_01_gt2r.h5'\n",
      "# 0.74% saturated, beam 3 (strong)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_09210_AIS_2020-21_waismeltregions_byrd_ATL03_20210111010129_02751011_005_01_gt2l.h5'\n",
      "# 0.73% saturated, beam 4 (weak)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_09998_AIS_2018-19_waismeltregions_nimrod_ATL03_20190112112541_02290211_005_01_gt2r.h5'\n",
      "# 0.73% saturated, beam 4 (weak)\n",
      "#fn = 'WAIS_2018-22-meltregions/lake_09475_AIS_2021-22_waismeltregions_nimrod_ATL03_20211221220536_13811311_005_01_gt2r.h5'\n"
     ]
    }
   ],
   "source": [
    "select_beam = -1\n",
    "perc_sat = dfs_all.groupby('lake_nr')['is_saturated'].mean()\n",
    "beamnr = dfs_all.groupby('lake_nr')['beam_number'].first()\n",
    "beamstr = dfs_all.groupby('lake_nr')['beam_strength'].first()\n",
    "df_perc_sat = pd.DataFrame({'perc_sat': perc_sat, 'beam_nr': beamnr, 'beam_str': beamstr, 'file': filelist})\n",
    "if select_beam > 0: df_perc_sat = df_perc_sat[df_perc_sat.beam_nr==select_beam]\n",
    "df_perc_sat.sort_values(by='perc_sat', inplace=True, ascending=False)\n",
    "for i in range(10):\n",
    "    print('# %.2f%% saturated, beam %i (%s)'%(df_perc_sat.iloc[i].perc_sat, df_perc_sat.iloc[i].beam_nr, df_perc_sat.iloc[i].beam_str))\n",
    "    print('#fn = \\'%s\\'' % df_perc_sat.iloc[i].file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f356b3eb-c1c9-4497-8d41-2f894271d6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>beam_nr</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>perc_sat</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beam_str</th>\n",
       "      <td>strong</td>\n",
       "      <td>weak</td>\n",
       "      <td>strong</td>\n",
       "      <td>weak</td>\n",
       "      <td>strong</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "beam_nr        1     2       3     4       5     6\n",
       "perc_sat     4.0   4.4     8.0  11.4     4.4   5.2\n",
       "beam_str  strong  weak  strong  weak  strong  weak"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp = df_perc_sat.groupby('beam_nr')\n",
    "pd.DataFrame([grp['perc_sat'].mean().round(3)*100,  grp['beam_str'].first()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e2a9dfc-c1ac-47d2-952c-bfeb6be23006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0a7abc912c4774a63d234671c3569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# histogram binning for afterpulse peaks vs. elevation\n",
    "bins = np.arange(start=thresh_lower, stop=thresh_upper, step=bin_h)\n",
    "mids = bins[:-1] + 0.5 * bin_h\n",
    "smooth = int(smooth_h/bin_h)\n",
    "if smooth %2 == 0: smooth += 1\n",
    "def get_histograms(ph_heights):\n",
    "    hist_h = np.histogram(ph_heights, bins=bins)\n",
    "    hist_h_smooth = np.array(pd.Series(hist_h[0]).rolling(smooth,center=True,min_periods=1).mean())\n",
    "    hist_h_plot = np.array(hist_h[0]) / hist_h_smooth.max()\n",
    "    hist_h_smooth /= hist_h_smooth.max()\n",
    "    return hist_h_plot, hist_h_smooth\n",
    "\n",
    "hist_h_all, hist_h_smooth_all = get_histograms(dfs.h)\n",
    "hist_h_sat, hist_h_smooth_sat = get_histograms(df_saturated.h)\n",
    "hist_h_sat_adjusted, hist_h_smooth_sat_adjusted = get_histograms(df_saturated.h_relative_to_saturated_peak)\n",
    "\n",
    "hist_h_issat = np.histogram(dfs.h_relative_to_saturated_peak[dfs.is_saturated], bins=bins)\n",
    "hist_h_nosat = np.histogram(dfs.h[~dfs.is_saturated], bins=bins)\n",
    "hist_h_issat = hist_h_issat[0] / np.sum(dfs.is_saturated)\n",
    "hist_h_nosat = hist_h_nosat[0] / np.sum(~dfs.is_saturated)\n",
    "hist_h_issat_smooth = np.array(pd.Series(hist_h_issat).rolling(smooth,center=True,min_periods=1).mean())\n",
    "hist_h_nosat_smooth = np.array(pd.Series(hist_h_nosat).rolling(smooth,center=True,min_periods=1).mean())\n",
    "normval = hist_h_issat_smooth.max()\n",
    "hist_h_issat /= normval\n",
    "hist_h_nosat /= normval\n",
    "hist_h_issat_smooth /= normval\n",
    "hist_h_nosat_smooth /= normval\n",
    "\n",
    "# get the peak elevations from data\n",
    "dist_m = 0.2\n",
    "xstep = bin_h\n",
    "hvals = mids[mids<0.2]\n",
    "dens_nosat = np.log10(hist_h_nosat_smooth[mids<0.2]/np.sum(hist_h_nosat_smooth[mids<0.2]))\n",
    "\n",
    "dens_sat = np.log10(hist_h_smooth_sat_adjusted[mids<0.2]/np.sum(hist_h_smooth_sat_adjusted[mids<0.2])) - dens_nosat\n",
    "hvals = mids[mids<0.2]\n",
    "peaks, props = find_peaks(dens_sat, distance=np.round(dist_m/xstep), prominence=0.01)\n",
    "props['idx'] = peaks\n",
    "props['elev'] = np.round(hvals[peaks],2)\n",
    "props['height'] = dens_sat[peaks]\n",
    "df_sat = pd.DataFrame(props)\n",
    "df_sat.reset_index(drop=True, inplace=True)\n",
    "df_sat.sort_values(by='prominences',ascending=False, ignore_index=True, inplace=True)\n",
    "df_sat = df_sat.iloc[:6]\n",
    "df_sat.sort_values(by='elev',ascending=False, ignore_index=True, inplace=True)\n",
    "\n",
    "dens_all = np.log10(hist_h_smooth_sat[mids<0.2]/np.sum(hist_h_smooth_sat_adjusted[mids<0.2])) - dens_nosat\n",
    "hvals = mids[mids<0.2]\n",
    "peaks, props = find_peaks(dens_all, distance=np.round(dist_m/xstep), prominence=0.01)\n",
    "props['idx'] = peaks\n",
    "props['elev'] = np.round(hvals[peaks],2)\n",
    "props['height'] = dens_all[peaks]\n",
    "df_all = pd.DataFrame(props)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.sort_values(by='prominences',ascending=False, ignore_index=True, inplace=True)\n",
    "df_all = df_all.iloc[:6]\n",
    "df_all.sort_values(by='elev',ascending=False, ignore_index=True, inplace=True)\n",
    "\n",
    "df_pks_info['h_lake'] = df_all.elev\n",
    "df_pks_info['h_pulse'] = df_sat.elev\n",
    "\n",
    "fig, (ax,ax2) = plt.subplots(ncols=2, figsize=[9, 5], dpi=100)\n",
    "ylms = (thresh_lower, thresh_upper)\n",
    "xlim_ax2 = (1e-5,10)\n",
    "\n",
    "# histogram showing peaks for specular returns\n",
    "ax.scatter(hist_h_all, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15)\n",
    "ax.plot(hist_h_smooth_all, mids, 'k-', lw=1)\n",
    "ax.set_xlabel('normalized photon counts')\n",
    "ax.set_ylabel('elevation relative to lake surface')\n",
    "ax.set_xlim(xlim_ax2)\n",
    "ax.set_ylim(ylms)\n",
    "ax.set_xscale('log')\n",
    "for i in range(len(df_pks_info)):\n",
    "    thispk = df_pks_info.iloc[i]\n",
    "    thispeak_height = hist_h_smooth_all[np.argmin(np.abs(mids-thispk.h_lake))]\n",
    "    ax.plot([xlim_ax2[0], thispeak_height], [thispk.h_lake]*2, color=thispk.color, ls=thispk.ls, zorder=-1000)\n",
    "    if i == 0:\n",
    "        ax.text(1.1*xlim_ax2[0], thispk.h_lake, 'lake surface', color=thispk.color, ha='left', va='bottom')\n",
    "    else:\n",
    "        ax.text(thispeak_height, thispk.h_lake, '    %.2f m' % thispk.h_lake, color=thispk.color, weight='bold', va='center')\n",
    "\n",
    "\n",
    "ax2.scatter(hist_h_sat_adjusted, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15)\n",
    "ax2.plot(hist_h_smooth_sat_adjusted, mids, 'k-', lw=1)\n",
    "# ax2.plot(hist_h_issat_smooth, mids, 'r-', lw=2)\n",
    "# ax2.plot(hist_h_nosat_smooth, mids, 'k-', lw=1, zorder=-100)\n",
    "# ax2.scatter(hist_h_issat, mids, s=3, color='red', lw=0.5, edgecolors='none', alpha=0.15, zorder=-200)\n",
    "# ax2.scatter(hist_h_nosat, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15, zorder=-300)\n",
    "\n",
    "ax2.set_xlabel('normalized photon counts (saturated pulses only)')\n",
    "ax2.set_ylabel('elevation relative to saturated surface return')\n",
    "ax2.set_xlim(xlim_ax2)\n",
    "ax2.set_ylim(ylms)\n",
    "ax2.set_xscale('log')\n",
    "for i in range(len(df_pks_info)):\n",
    "    thispk = df_pks_info.iloc[i]\n",
    "    thispeak_height = hist_h_smooth_sat_adjusted[np.argmin(np.abs(mids-thispk.h_pulse))]\n",
    "    ax2.plot([xlim_ax2[0], thispeak_height], [thispk.h_pulse]*2, color=thispk.color, ls=thispk.ls, zorder=-1000)\n",
    "    if i == 0:\n",
    "        ax2.text(1.1*xlim_ax2[0], thispk.h_pulse, 'saturated surface return', color=thispk.color, ha='left', va='bottom')\n",
    "    else:\n",
    "        ax2.text(thispeak_height, thispk.h_pulse, '    %.2f m' % thispk.h_pulse, color=thispk.color, weight='bold', va='center')\n",
    "        \n",
    "fig.suptitle('ICESat-2 afterpulses over %i melt lakes (beams: %s)' % (n_lakes, beam_select), y=0.98, fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "figname = 'figs_afterpulses/ICESat-2-afterpulses-melt-lakes-%s-beams.jpg' % strength\n",
    "# fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1619c26b-2685-4356-a866-fe71c8ff68f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdabe7427244c1ea8da2588220e7a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-0.75, 0.75)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get peaks\n",
    "dist_m = 0.2\n",
    "xstep = bin_h\n",
    "hvals = mids[mids<0.2]\n",
    "dens_nosat = np.log10(hist_h_nosat_smooth[mids<0.2]/np.sum(hist_h_nosat_smooth[mids<0.2]))\n",
    "\n",
    "dens_sat = np.log10(hist_h_smooth_sat_adjusted[mids<0.2]/np.sum(hist_h_smooth_sat_adjusted[mids<0.2])) - dens_nosat\n",
    "peaks, props = find_peaks(dens_sat, distance=np.round(dist_m/xstep), prominence=0.01)\n",
    "props['idx'] = peaks\n",
    "props['elev'] = np.round(hvals[peaks],2)\n",
    "props['height'] = dens_sat[peaks]\n",
    "df_sat = pd.DataFrame(props)\n",
    "df_sat.reset_index(drop=True, inplace=True)\n",
    "df_sat.sort_values(by='prominences',ascending=False, ignore_index=True, inplace=True)\n",
    "df_sat = df_sat.iloc[:6]\n",
    "\n",
    "dens_all = np.log10(hist_h_smooth_sat[mids<0.2]/np.sum(hist_h_smooth_sat_adjusted[mids<0.2])) - dens_nosat\n",
    "peaks, props = find_peaks(dens_all, distance=np.round(dist_m/xstep), prominence=0.01)\n",
    "props['idx'] = peaks\n",
    "props['elev'] = np.round(hvals[peaks],2)\n",
    "props['height'] = dens_all[peaks]\n",
    "df_all = pd.DataFrame(props)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.sort_values(by='prominences',ascending=False, ignore_index=True, inplace=True)\n",
    "df_all = df_all.iloc[:6]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[9, 5], dpi=100)\n",
    "ax.plot(hvals, dens_all, 'k-')\n",
    "ax.plot(df_all.elev, df_all.height, 'x', c='r')\n",
    "ax.plot(hvals, dens_sat, 'b-')\n",
    "ax.plot(df_sat.elev, df_sat.height, '.', c='r')\n",
    "ax.set_ylim((-0.75,0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2d43f-7eb8-43db-933c-2a460a782c48",
   "metadata": {},
   "source": [
    "# Plot comparison between weak and strong beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fad65d-fe91-4112-8a66-ccc99f473e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "for strength in ['all', 'weak', 'strong']:\n",
    "    df_list = []\n",
    "    n_lakes = 0\n",
    "    \n",
    "    for filename in filelist:\n",
    "        with open(filename, 'rb') as f:\n",
    "            lk = pickle.load(f)\n",
    "\n",
    "        surf_elev = lk['surface_elevation']\n",
    "        df = lk['photon_data']\n",
    "        beam_strength = lk['beam_strength']\n",
    "        dfs = df[(df.h < (thresh_upper+surf_elev)) & (df.h > (thresh_lower+surf_elev))].copy()\n",
    "        dfs['h'] = dfs.h - surf_elev\n",
    "        dfs['pulseid'] = dfs.apply(lambda row: 1000*row.mframe+row.ph_id_pulse, axis=1)\n",
    "        dfs = dfs.set_index('pulseid')\n",
    "        df_surf_photons = dfs[(dfs.h >= -0.175) & (dfs.h < 0.275)].copy()\n",
    "        df_pulses = group_by_pulse(df_surf_photons, smooth_pulse, beam_strength)\n",
    "        df_join = dfs.join(df_pulses, how='left', rsuffix='_pulse')\n",
    "        df_join['h_relative_to_saturated_peak'] = df_join.h - df_join.h_pulse\n",
    "        if (strength not in ['weak','strong']) | (strength == beam_strength):\n",
    "            n_lakes += 1\n",
    "            df_list.append(df_join)\n",
    "\n",
    "    dfs = pd.concat(df_list)\n",
    "    df_saturated = dfs[dfs.ph_count_smooth > 0.99]\n",
    "    # df_saturated = dfs[dfs.ph_count > 0.99]\n",
    "\n",
    "    # histogram binning for afterpulse peaks vs. elevation\n",
    "    bins = np.arange(start=thresh_lower, stop=thresh_upper, step=bin_h)\n",
    "    mids = bins[:-1] + 0.5 * bin_h\n",
    "    smooth = int(smooth_h/bin_h)\n",
    "    if smooth %2 == 0: smooth += 1\n",
    "    def get_histograms(ph_heights):\n",
    "        hist_h = np.histogram(ph_heights, bins=bins)\n",
    "        hist_h_smooth = np.array(pd.Series(hist_h[0]).rolling(smooth,center=True,min_periods=1).mean())\n",
    "        hist_h_plot = np.array(hist_h[0]) / hist_h_smooth.max()\n",
    "        hist_h_smooth /= hist_h_smooth.max()\n",
    "        return hist_h_plot, hist_h_smooth\n",
    "\n",
    "    hist_h_all, hist_h_smooth_all = get_histograms(dfs.h)\n",
    "    hist_h_sat, hist_h_smooth_sat = get_histograms(df_saturated.h)\n",
    "    hist_h_sat_adjusted, hist_h_smooth_sat_adjusted = get_histograms(df_saturated.h_relative_to_saturated_peak)\n",
    "\n",
    "    fig, ax2 = plt.subplots(ncols=1, figsize=[5, 5], dpi=100)\n",
    "    ylms = (thresh_lower, thresh_upper)\n",
    "    xlim_ax2 = (1e-5,10)\n",
    "\n",
    "    # histogram showing peaks for specular returns\n",
    "    ax2.scatter(hist_h_sat_adjusted, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15)\n",
    "    ax2.plot(hist_h_smooth_sat_adjusted, mids, 'k-', lw=1)\n",
    "    ax2.set_xlabel('normalized photon counts (saturated pulses only)')\n",
    "    ax2.set_ylabel('elevation relative to saturated surface return')\n",
    "    ax2.set_xlim(xlim_ax2)\n",
    "    ax2.set_ylim(ylms)\n",
    "    ax2.set_xscale('log')\n",
    "    for i in range(len(df_pks_info)):\n",
    "        thispk = df_pks_info.iloc[i]\n",
    "        thispeak_height = hist_h_smooth_sat_adjusted[np.argmin(np.abs(mids-thispk.h_pulse))]\n",
    "        ax2.plot([xlim_ax2[0], thispeak_height], [thispk.h_pulse]*2, color=thispk.color, ls=thispk.ls, zorder=-1000)\n",
    "        if i == 0:\n",
    "            ax2.text(1.1*xlim_ax2[0], thispk.h_pulse, 'saturated surface return', color=thispk.color, ha='left', va='bottom')\n",
    "        else:\n",
    "            ax2.text(thispeak_height, thispk.h_pulse, '    %.2f m' % thispk.h_pulse, color=thispk.color, weight='bold', va='center')\n",
    "\n",
    "    ax2.text(0.7, 0.95, '%s beams' % strength, ha='center', va='center', fontsize=10, weight='bold', color='red', transform=ax2.transAxes)\n",
    "    txt = 'ICESat-2 afterpulses over %i melt lakes\\n' % n_lakes\n",
    "    ax2.set_title(txt, fontsize=10)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    figname = 'figs_afterpulses/ICESat2_meltlake-afterpulses-%s-beams.jpg' % strength\n",
    "    fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752cb32-e952-468f-a694-be5cb6189b3c",
   "metadata": {},
   "source": [
    "# Example of second return removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7b2ab-e633-4c5c-b6e4-e888c566920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.signal import find_peaks\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gsp\n",
    "plt.rcParams.update({'font.size': 5})\n",
    "\n",
    "def group_by_pulse(df_in, smoothing, beam_strength):\n",
    "    thegroup = df_in.groupby('pulseid')\n",
    "    df_grouped = thegroup[['xatc', 'h']].mean()\n",
    "    norm_factor = 4 if beam_strength == 'weak' else 16\n",
    "    df_grouped['ph_count'] = thegroup['h'].count() / norm_factor\n",
    "    df_grouped['ph_count_smooth'] = np.array(df_grouped.ph_count.rolling(smoothing,center=True,min_periods=1).mean())\n",
    "    return df_grouped\n",
    "\n",
    "def get_histograms(ph_heights, bins, smooth):\n",
    "    hist_h = np.histogram(ph_heights, bins=bins)\n",
    "    hist_h_smooth = np.array(pd.Series(hist_h[0]).rolling(smooth,center=True,min_periods=1).mean())\n",
    "    hist_h_plot = np.array(hist_h[0]) / hist_h_smooth.max()\n",
    "    hist_h_smooth /= hist_h_smooth.max()\n",
    "    return hist_h_plot, hist_h_smooth\n",
    "\n",
    "def plot_counts(df_in, df_pks_info, thisax, alph):\n",
    "    thisax.scatter(df_in.xatc, df_in.ph_count, s=3, color=df_pks_info.iloc[i].color, edgecolors='none', alpha=alph)\n",
    "    thisax.plot(df_in.xatc, df_in.ph_count_smooth, color=df_pks_info.iloc[i].color, ls=df_pks_info.iloc[i].ls, lw=1)\n",
    "    \n",
    "def hex2rgb(hex_str):\n",
    "    return list(np.array([int(hex_str.lstrip('#')[i:i+2], 16) for i in (0, 2, 4)],dtype=float)/255)\n",
    "\n",
    "thresh_upper = 1.0\n",
    "thresh_lower = -5.0\n",
    "bin_h = 0.01\n",
    "smooth_h = 0.1\n",
    "extent_buffer = 20.0\n",
    "smooth_pulse = 1 # makes it roughly the footprint size\n",
    "smooth_pulse_plot = 20 # to get an idea from the plot\n",
    "saturation_threshold = 0.97\n",
    "removal_width_adjust = 0.5\n",
    "surf_saturation_detection_limits = [-0.3,0.5]\n",
    "surf_saturation_detection_step = 0.025\n",
    "\n",
    "\n",
    "cols_pk = ['#000000','#CD104D', '#E14D2A', '#FD841F', '#8FE3CF', '#256D85']\n",
    "desc_pk = ['lake surface', 'dead time (1st)', 'dead time (2nd)', 'dead time (3rd)',\n",
    "           'internal reflection (1st)', 'internal reflection (2nd)']\n",
    "lsty_pk = ['-', '-', '--', ':', '-', ':']\n",
    "elev_pk_lake = [0.0, -0.55, -0.845, -1.36, -2.4, -4.2]\n",
    "elev_pk_puls = [0.0, -0.55, -0.91, -1.465, -2.4, -4.2]\n",
    "widths_pk = [0.225, 0.225, 0.225, 0.225, 0.3, 0.3] # 0.225 m on each side makes it 0.45 total, which is the dead-time for ATLAS\n",
    "df_pks_info = pd.DataFrame({'h_lake': elev_pk_lake, 'h_pulse': elev_pk_puls, 'width': widths_pk, \n",
    "                            'description': desc_pk, 'color': cols_pk, 'ls': lsty_pk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b56d4-383b-402c-a3a4-1fe1416057b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_perc_sat = 1-np.array(perc_sat_list)\n",
    "idxs = np.argsort(inv_perc_sat)\n",
    "sortedlist = np.sort(inv_perc_sat)\n",
    "fn_sorted = [filelist[idx] for idx in idxs]\n",
    "for i in range(3):\n",
    "    print ('%.3f, \\'%s\\'' % (1-sortedlist[i], fn_sorted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e51b2-2f9b-4922-974b-d499f8076320",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "searchfor = 'specular_test0001_'\n",
    "\n",
    "filelist = ['pickles/'+f for f in os.listdir('pickles') \\\n",
    "            if os.path.isfile(os.path.join('pickles', f)) & (searchfor in f)]\n",
    "\n",
    "filelist = ['pickles/testdata_01207_nivlisen_test_2020-01-16_0312_lake0002.pkl',\n",
    "            'pickles/testdata_07013_jakobshavn_test_2021-07-15_0338_lake0042.pkl',\n",
    "            'pickles/testdata_07485_amery_test_2019-01-02_0081_lake0031.pkl',\n",
    "            'pickles/testdata_04845_jakobshavn_test_2021-07-15_0338_lake0043.pkl',\n",
    "            'pickles/testdata_03749_79N_test_2019-08-22_0848_lake0002.pkl',\n",
    "            'pickles/testdata_06401_jakobshavn_test_2021-07-15_0338_lake0045.pkl',\n",
    "            'pickles/testdata_07215_amery_test_2019-01-02_0081_lake0009.pkl',\n",
    "            'pickles/testdata_09130_jakobshavn_test_2021-08-09_0719_lake0056.pkl',\n",
    "            'pickles/testdata_09318_GrIS-NE-test_2021-08-22_0909_lake0005.pkl',\n",
    "            'pickles/testdata_08171_jakobshavn_test_2021-07-15_0338_lake0016.pkl',\n",
    "            'pickles/testdata_09748_GrIS-NE-test_2021-08-22_0909_lake0001.pkl',\n",
    "            'pickles/testdata_09203_GrIS-NE-test_2021-08-22_0909_lake0005.pkl',\n",
    "            'pickles/testdata_08324_jakobshavn_test_2021-08-02_0605_lake0032.pkl',\n",
    "            'pickles/testdata_09578_amery_test_2019-01-02_0081_lake0048.pkl']\n",
    "\n",
    "for filename in filelist:\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        lk = pickle.load(f)\n",
    "\n",
    "    surf_elev = lk['surface_elevation']\n",
    "    df = lk['photon_data']\n",
    "    beam_strength = lk['beam_strength']\n",
    "\n",
    "    df['is_afterpulse'] = False\n",
    "    selector_afterpulse_search = (df.h < (thresh_upper+surf_elev)) & (df.h > (thresh_lower+surf_elev))\n",
    "    dfs = df[selector_afterpulse_search].copy()\n",
    "    dfs['h'] = dfs.h - surf_elev\n",
    "    dfs['pulseid'] = dfs.apply(lambda row: 1000*row.mframe+row.ph_id_pulse, axis=1)\n",
    "    dfs['ph_index'] = dfs.index\n",
    "    dfs = dfs.set_index('pulseid')\n",
    "    pkinfo = df_pks_info.iloc[0]\n",
    "    photon_df = dfs[(dfs.h >= (pkinfo.h_pulse - 0.2)) & (dfs.h < (pkinfo.h_pulse + pkinfo.width))]\n",
    "    df_pulses = group_by_pulse(photon_df,smooth_pulse,beam_strength)\n",
    "    df_join = dfs.join(df_pulses, how='left', rsuffix='_pulse')\n",
    "    df_join['pulseid'] = dfs.index\n",
    "    df_join = df_join.set_index('ph_index')\n",
    "    df_join['h_relative_to_saturated_peak'] = df_join.h - df_join.h_pulse\n",
    "    df_join['is_saturated'] = df_join.ph_count_smooth >= saturation_threshold \n",
    "\n",
    "    # histogram binning for plotting afterpulse peaks vs. elevation\n",
    "    bins = np.arange(start=thresh_lower, stop=thresh_upper, step=bin_h)\n",
    "    mids = bins[:-1] + 0.5 * bin_h\n",
    "    smooth = int(smooth_h/bin_h)\n",
    "    if smooth %2 == 0: smooth += 1\n",
    "    hist_h, hist_h_smooth = get_histograms(df_join.h_relative_to_saturated_peak, bins, smooth)\n",
    "\n",
    "    # get saturation ratio for each possible afterpulse\n",
    "    pulse_dfs = []\n",
    "    for i in range(len(df_pks_info)):\n",
    "        pkinfo = df_pks_info.iloc[i]\n",
    "        photon_df = df_join[(df_join.h >= (pkinfo.h_pulse - pkinfo.width)) & (df_join.h < (pkinfo.h_pulse + pkinfo.width))]\n",
    "        pulse_dfs.append(group_by_pulse(photon_df, smooth_pulse_plot, beam_strength))\n",
    "\n",
    "    # indicate possible afterpulses\n",
    "    for i in range(1, len(df_pks_info)):\n",
    "        pkinfo = df_pks_info.iloc[i]\n",
    "        is_in_afterpulse_range = (df_join.h >= (pkinfo.h_lake - pkinfo.width)) & (df_join.h < (pkinfo.h_lake + pkinfo.width))\n",
    "        is_afterpulse = is_in_afterpulse_range & df_join.is_saturated\n",
    "        df_join.loc[is_afterpulse, 'is_afterpulse'] = True\n",
    "\n",
    "    df.loc[selector_afterpulse_search, 'is_afterpulse'] = df_join.is_afterpulse\n",
    "\n",
    "    # get saturated segments that are continuous for longer than x meters, for plotting\n",
    "    current_list, saturat_segs, i = [], [], 0\n",
    "    min_length_saturated_plot = 15.0 # roughly the footprint size\n",
    "    while i < len(df_join):\n",
    "        if df_join.is_saturated.iloc[i]:\n",
    "            current_list.append(i)\n",
    "        elif len(current_list) > 1:\n",
    "            saturat_segs.append([df_join.xatc.iloc[current_list[0]], df_join.xatc.iloc[current_list[-1]]])\n",
    "            current_list = []\n",
    "        i += 1 \n",
    "    saturat_segs_xatc = [item for item in saturat_segs if np.abs(item[-1] - item[0]) > min_length_saturated_plot]\n",
    "\n",
    "    df_plot_afterpulses = df[df.is_afterpulse]\n",
    "    df_plot_no_afterpulses = df[~df.is_afterpulse]\n",
    "\n",
    "    #__________________________________________________________________\n",
    "    # make the figure\n",
    "    #fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows=2, ncols=2, figsize=[9, 5], dpi=100)\n",
    "    fig = plt.figure(figsize=[9, 5], dpi=100)\n",
    "    gs = fig.add_gridspec(4, 4)\n",
    "    ax1 = fig.add_subplot(gs[ :2,  :2])\n",
    "    ax2 = fig.add_subplot(gs[ :2, 2:4])\n",
    "    ax3 = fig.add_subplot(gs[2:4,  :2])\n",
    "    ax4 = fig.add_subplot(gs[2:4, 2:3])\n",
    "    ax5 = fig.add_subplot(gs[2:4, 3:4])\n",
    "\n",
    "    if len(lk['surface_extent_detection'])>0:\n",
    "        xlms = (lk['surface_extent_detection'][0][0]-extent_buffer, lk['surface_extent_detection'][-1][-1]+extent_buffer)\n",
    "    else:\n",
    "        xlms = (df_join.xatc.min(), df_join.xatc.max())\n",
    "    xlim_ax2 = (5e-5,2)\n",
    "    ylms = (thresh_lower, thresh_upper)\n",
    "    lbpd = 0 # axis label padding\n",
    "    lbsz = 7 # axis label font size\n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # ax1: selected photons\n",
    "    ax = ax1\n",
    "    pscatt = ax.scatter(dfs.xatc, dfs.h, s=1, c='k', alpha=1, edgecolors='none',label='ATL03 data')\n",
    "    ax.scatter(df_plot_afterpulses.xatc, df_plot_afterpulses.h-surf_elev, s=1.5, c='k', edgecolors='none')\n",
    "    ax.scatter(df_plot_afterpulses.xatc, df_plot_afterpulses.h-surf_elev, s=1, c='r', edgecolors='none')\n",
    "\n",
    "    # title and labels\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(which='major', axis='both', color='gray', lw=0.5, alpha=0.3)\n",
    "    ax.set_xlim(xlms)\n",
    "    ax.set_ylim(ylms)\n",
    "    bbox = {'fc':(1,1,1,0.75), 'ec':(1,1,1,0), 'pad':1}\n",
    "    ax.text(0.5, 0.95, 'ATL03 photon data', va='center', ha='center', fontsize=8, weight='bold', bbox=bbox, transform=ax.transAxes)\n",
    "    ax.set_xlabel('along-track distance [m]', fontsize=lbsz, labelpad=lbpd)\n",
    "    ax.set_ylabel('elevation realtive to lake surface [m]', fontsize=lbsz, labelpad=lbpd)\n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # ax2: histogram showing peaks for specular returns\n",
    "    ax = ax2\n",
    "    ax.scatter(hist_h, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15)\n",
    "    ax.plot(hist_h_smooth, mids, 'k-', lw=1)\n",
    "    ax.set_xlim(xlim_ax2)\n",
    "    ax.set_ylim(ylms)\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(which='major', axis='y', color='gray', lw=0.5, alpha=0.3)\n",
    "    for i in range(len(df_pks_info)):\n",
    "        thispk = df_pks_info.iloc[i]\n",
    "        thispeak_height = hist_h_smooth[np.argmin(np.abs(mids-thispk.h_pulse))]\n",
    "        ax.plot([xlim_ax2[0], thispeak_height], [thispk.h_pulse]*2, color=thispk.color, ls=thispk.ls, zorder=-1000, lw=0.5)\n",
    "        if i == 0:\n",
    "            ax.text(1.1*xlim_ax2[0], thispk.h_pulse, 'saturated surface return', color=thispk.color, \n",
    "                    ha='left', va='bottom', fontsize=7)\n",
    "        else:\n",
    "            ax.text(thispeak_height, thispk.h_pulse, '    %.2f m' % thispk.h_pulse, color=thispk.color, \n",
    "                    weight='bold', va='center', fontsize=7)\n",
    "    ax.set_xlabel('normalized photon counts (saturated pulses only)', fontsize=lbsz, labelpad=lbpd)\n",
    "    ax.set_ylabel('elevation relative to saturated\\npulse surface peak [m]', fontsize=lbsz, labelpad=lbpd)\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    # plot photon counts per shot\n",
    "    alph = 0.1\n",
    "    ax = ax3\n",
    "\n",
    "    for i,this_peak_pulse_df in enumerate(pulse_dfs):\n",
    "        plot_counts(this_peak_pulse_df, df_pks_info, ax, alph)\n",
    "\n",
    "    ax.set_xlim(xlms)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(which='major', axis='x', color='gray', lw=0.5, alpha=0.3)\n",
    "    ax.set_yticks([])\n",
    "    if beam_strength == 'strong':\n",
    "        yts = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        ax.set_ylim((1/16,1.05))\n",
    "    if beam_strength == 'weak':\n",
    "        ax.set_ylim((1/4,1.05))\n",
    "        yts = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    ytl = ['%.1f'%yt for yt in yts]\n",
    "    ax.set_yticks(yts)\n",
    "    ax.set_yticklabels(ytl)\n",
    "    ax.set_xlabel('along-track distance [m]', fontsize=lbsz, labelpad=lbpd)\n",
    "    ax.set_ylabel('% saturated per pulse', fontsize=lbsz, labelpad=lbpd)\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    # the full lake, for reference\n",
    "    rng = np.abs(surf_elev-np.min(lk['detection_2nd_returns']['h']))\n",
    "    yls = (surf_elev-1.1*rng, surf_elev+0.3*rng)\n",
    "\n",
    "    # all the data \n",
    "    ax = ax4\n",
    "    ax.scatter(df.xatc, df.h, c='k', s=1, alpha=1, edgecolors='none')\n",
    "    ax.set_xlim((df.xatc.min(),df.xatc.max()))\n",
    "    ax.set_ylim(yls)\n",
    "    bbox = {'fc':(1,1,1,0.75), 'ec':(1,1,1,0), 'pad':1}\n",
    "    ax.text(0.5, 0.95, 'all photons', va='center', ha='center', fontsize=8, weight='bold', bbox=bbox, transform=ax.transAxes)\n",
    "    ax.set_xlabel('along-track distance [m]', fontsize=lbsz-1, labelpad=lbpd)\n",
    "    ax.set_ylabel('elevation above geoid [m]', fontsize=lbsz, labelpad=lbpd)\n",
    "\n",
    "    # now with afterpulses removed\n",
    "    ax = ax5\n",
    "    ax.scatter(df_plot_no_afterpulses.xatc, df_plot_no_afterpulses.h, c='k', s=1, alpha=1, edgecolors='none')\n",
    "    ax.set_xlim((df.xatc.min(),df.xatc.max()))\n",
    "    ax.set_ylim(yls)\n",
    "    ax.set_yticks([])\n",
    "    bbox = {'fc':(1,1,1,0.75), 'ec':(1,1,1,0), 'pad':1}\n",
    "    ax.text(0.5, 0.95, 'afterpulses removed', va='center', ha='center', fontsize=8, weight='bold', bbox=bbox, transform=ax.transAxes)\n",
    "    ax.set_xlabel('along-track distance [m]', fontsize=lbsz-1, labelpad=lbpd)\n",
    "\n",
    "    #_________________________________________________\n",
    "    # highlight saturated areas\n",
    "    alph = 0.3\n",
    "    sat_color = [1, 1, 0]\n",
    "    for seg_xatc in saturat_segs_xatc:\n",
    "        # highlight saturation on ax3\n",
    "        yls = ax3.get_ylim()\n",
    "        xy, width, height = (seg_xatc[0], yls[0]), np.abs(seg_xatc[-1]-seg_xatc[0]), yls[-1]-yls[0]\n",
    "        rct = Rectangle(xy, width, height, ec=(1,1,1,0), fc=sat_color+[alph], zorder=-1000)\n",
    "        ax3.add_patch(rct)\n",
    "        # highlight saturation on ax1, where they overlap\n",
    "        yls = ax1.get_ylim()\n",
    "        xy, width, height = (seg_xatc[0], yls[0]), np.abs(seg_xatc[-1]-seg_xatc[0]), yls[-1]-yls[0]\n",
    "        rct = Rectangle(xy, width, height, ec=(1,1,1,0), fc=sat_color+[alph], zorder=-1000)\n",
    "        ax1.add_patch(rct)\n",
    "\n",
    "    # highlight peak ranges \n",
    "    afterpulse_hdls = []\n",
    "    for i in range(1,len(df_pks_info)):\n",
    "        info = df_pks_info.iloc[i]\n",
    "        # highlight ranges on ax2\n",
    "        lower, upper = info.h_pulse-info.width*removal_width_adjust, info.h_pulse+info.width*removal_width_adjust\n",
    "        selector = (mids > lower) & (mids < upper)\n",
    "        ys_fill, x1_fill = mids[selector], hist_h_smooth[selector]\n",
    "        x2_fill = [xlim_ax2[0]]*len(ys_fill)\n",
    "        thiscolor = hex2rgb(info.color) + [alph]\n",
    "        hdl = ax2.fill_betweenx(ys_fill, x1_fill, x2_fill, ec=(1,1,1,0), fc=thiscolor, zorder=-10000, label=info.description)\n",
    "        afterpulse_hdls.append(hdl)\n",
    "        # highlight ranges on ax1, where they overlap\n",
    "        xy, width, height = (xlms[0],info.h_lake-info.width*removal_width_adjust), xlms[1]-xlms[0], info.width*removal_width_adjust*2\n",
    "        thiscolor = hex2rgb(info.color) + [alph]\n",
    "        rct = Rectangle(xy, width, height, ec=(1,1,1,0), fc=thiscolor, zorder=-1000)\n",
    "        ax1.add_patch(rct)\n",
    "\n",
    "    ax2.legend(handles=afterpulse_hdls, loc='lower right')\n",
    "\n",
    "    #_________________________________________________\n",
    "    # final figure stuff\n",
    "    txt  = 'REMOVAL OF ICESat-2 SPECULAR RETURN AFTERPULSES OVER A MELT LAKE\\n'\n",
    "    txt += 'location: %s, %s (area: %s) | ' % (lk['lat_str'], lk['lon_str'], lk['polygon_name'])\n",
    "    txt += 'time: %s UTC | surface elevation: %.2f m\\n' % (lk['date_time'], lk['surface_elevation'])\n",
    "    txt += 'RGT %s %s cycle %i | ' % (lk['rgt'], lk['gtx'].upper(), lk['cycle_number'])\n",
    "    txt += 'beam %i (%s, %s spacecraft orientation) | ' % (lk['beam_number'], lk['beam_strength'], lk['sc_orient'])\n",
    "    txt += 'granule ID: %s' % lk['granule_id']\n",
    "    fig.suptitle(txt, y=0.98, fontsize=9)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    figname = 'figs_afterpulses/afterpulse_removal_%s.jpg' % filename[filename.rfind('/')+1:].replace('.pkl', '')\n",
    "    fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c2651-3750-4d08-8444-f2cc5b3c55c7",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e8ddd-75b1-46c2-b2cc-f83d8cc13d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.signal import find_peaks\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "def group_by_pulse(df_in, smoothing, beam_strength):\n",
    "    thegroup = df_in.groupby('pulseid')\n",
    "    df_grouped = thegroup[['xatc', 'h']].mean()\n",
    "    norm_factor = 4 if beam_strength == 'weak' else 16\n",
    "    df_grouped['ph_count'] = thegroup['h'].count() / norm_factor\n",
    "    df_grouped['ph_count_smooth'] = np.array(df_grouped.ph_count.rolling(smoothing,center=True,min_periods=1).mean())\n",
    "    return df_grouped\n",
    "\n",
    "def get_histograms(ph_heights, bins, smooth):\n",
    "    hist_h = np.histogram(ph_heights, bins=bins)\n",
    "    hist_h_smooth = np.array(pd.Series(hist_h[0]).rolling(smooth,center=True,min_periods=1).mean())\n",
    "    hist_h_plot = np.array(hist_h[0]) / hist_h_smooth.max()\n",
    "    hist_h_smooth /= hist_h_smooth.max()\n",
    "    return hist_h_plot, hist_h_smooth\n",
    "\n",
    "def plot_counts(df_in, df_pks_info, thisax, alph):\n",
    "    thisax.scatter(df_in.xatc, df_in.ph_count, s=3, color=df_pks_info.iloc[i].color, edgecolors='none', alpha=alph)\n",
    "    thisax.plot(df_in.xatc, df_in.ph_count_smooth, color=df_pks_info.iloc[i].color, ls=df_pks_info.iloc[i].ls)\n",
    "    \n",
    "thresh_upper = 1.0\n",
    "thresh_lower = -5.0\n",
    "bin_h = 0.01\n",
    "smooth_h = 0.1\n",
    "extent_buffer = 20.0\n",
    "smooth_pulse = 15 # makes it roughly the footprint size\n",
    "saturation_threshold = 0.95\n",
    "\n",
    "cols_pk = ['black','#CD104D', '#E14D2A', '#FD841F', '#8FE3CF', '#256D85']\n",
    "lsty_pk = ['-', '-', '--', ':', '-', ':']\n",
    "elev_pk_lake = [0.0, -0.55, -0.845, -1.36, -2.4, -4.2]\n",
    "elev_pk_puls = [0.0, -0.55, -0.91, -1.465, -2.4, -4.2]\n",
    "widths_pk = [0.225, 0.225, 0.225, 0.225, 0.3, 0.3] # 0.225 m on each side makes it 0.45 total, which is the dead-time for ATLAS\n",
    "df_pks_info = pd.DataFrame({'h_lake': elev_pk_lake, 'h_pulse': elev_pk_puls, 'width': widths_pk, 'color': cols_pk, 'ls': lsty_pk})\n",
    "\n",
    "plt.close('all')\n",
    "i = 50\n",
    "\n",
    "fn = 'pickles/specular%02i.pkl' % i\n",
    "with open(fn, 'rb') as f:\n",
    "    lk = pickle.load(f)\n",
    "\n",
    "surf_elev = lk['surface_elevation']\n",
    "df = lk['photon_data']\n",
    "beam_strength = lk['beam_strength']\n",
    "dfs = df[(df.h < (thresh_upper+surf_elev)) & (df.h > (thresh_lower+surf_elev))].copy()\n",
    "dfs['h'] = dfs.h - surf_elev\n",
    "dfs['pulseid'] = dfs.apply(lambda row: 1000*row.mframe+row.ph_id_pulse, axis=1)\n",
    "dfs['ph_index'] = dfs.index\n",
    "dfs = dfs.set_index('pulseid')\n",
    "pkinfo = df_pks_info.iloc[0]\n",
    "photon_df = dfs[(dfs.h >= (pkinfo.h_pulse - pkinfo.width)) & (dfs.h < (pkinfo.h_pulse + pkinfo.width))]\n",
    "df_pulses = group_by_pulse(photon_df,smooth_pulse,beam_strength)\n",
    "df_join = dfs.join(df_pulses, how='left', rsuffix='_pulse')\n",
    "df_join['pulseid'] = dfs.index\n",
    "df_join = df_join.set_index('ph_index')\n",
    "df_join['h_relative_to_saturated_peak'] = df_join.h - df_join.h_pulse\n",
    "df_join['is_saturated'] = df_join.ph_count_smooth >= saturation_threshold \n",
    "\n",
    "# histogram binning for plotting afterpulse peaks vs. elevation\n",
    "bins = np.arange(start=thresh_lower, stop=thresh_upper, step=bin_h)\n",
    "mids = bins[:-1] + 0.5 * bin_h\n",
    "smooth = int(smooth_h/bin_h)\n",
    "if smooth %2 == 0: smooth += 1\n",
    "# hist_h, hist_h_smooth = get_histograms(df_join.h_relative_to_saturated_peak[df_join.is_saturated], bins, smooth)\n",
    "\n",
    "hist_h_issat = np.histogram(df_join.h_relative_to_saturated_peak[df_join.is_saturated], bins=bins)\n",
    "hist_h_nosat = np.histogram(df_join.h[~df_join.is_saturated], bins=bins)\n",
    "hist_h_issat = hist_h_issat[0] / np.sum(df_join.is_saturated)\n",
    "hist_h_nosat = hist_h_nosat[0] / np.sum(~df_join.is_saturated)\n",
    "hist_h_issat_smooth = np.array(pd.Series(hist_h_issat).rolling(smooth,center=True,min_periods=1).mean())\n",
    "hist_h_nosat_smooth = np.array(pd.Series(hist_h_nosat).rolling(smooth,center=True,min_periods=1).mean())\n",
    "normval = hist_h_issat_smooth.max()\n",
    "hist_h_issat /= normval\n",
    "hist_h_nosat /= normval\n",
    "hist_h_issat_smooth /= normval\n",
    "hist_h_nosat_smooth /= normval\n",
    "\n",
    "# get saturation ratio for each possible afterpulse\n",
    "pulse_dfs = []\n",
    "for i in range(len(df_pks_info)):\n",
    "    pkinfo = df_pks_info.iloc[i]\n",
    "    photon_df = df_join[(df_join.h >= (pkinfo.h_pulse - pkinfo.width)) & (df_join.h < (pkinfo.h_pulse + pkinfo.width))]\n",
    "    pulse_dfs.append(group_by_pulse(photon_df,smooth_pulse,beam_strength))\n",
    "\n",
    "#__________________________________________________________________\n",
    "# make the figure\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[5, 5], dpi=100)\n",
    "xlms = (df_join.xatc.min(), df_join.xatc.max())\n",
    "xlim_ax2 = (5e-5,2)\n",
    "ylms = (thresh_lower, thresh_upper)\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "# ax: histogram showing peaks for specular returns\n",
    "ax.plot(hist_h_issat_smooth, mids, 'r-', lw=2)\n",
    "ax.plot(hist_h_nosat_smooth, mids, 'k-', lw=1, zorder=-100)\n",
    "ax.scatter(hist_h_issat, mids, s=3, color='red', lw=0.5, edgecolors='none', alpha=0.15, zorder=-200)\n",
    "ax.scatter(hist_h_nosat, mids, s=3, color='black', lw=0.5, edgecolors='none', alpha=0.15, zorder=-300)\n",
    "ax.set_xlabel('normalized photon counts (saturated pulses only)')\n",
    "ax.set_ylabel('elevation relative to saturated\\npulse surface peak [m]')\n",
    "ax.set_xlim(xlim_ax2)\n",
    "ax.set_ylim(ylms)\n",
    "ax.set_xscale('log')\n",
    "for i in range(len(df_pks_info)):\n",
    "    thispk = df_pks_info.iloc[i]\n",
    "    thispeak_height = hist_h_smooth[np.argmin(np.abs(mids-thispk.h_pulse))]\n",
    "    ax.plot([xlim_ax2[0], thispeak_height], [thispk.h_pulse]*2, color=thispk.color, ls=thispk.ls, zorder=-1000)\n",
    "    if i == 0:\n",
    "        ax.text(1.1*xlim_ax2[0], thispk.h_pulse, 'saturated surface return', color=thispk.color, ha='left', va='bottom')\n",
    "    else:\n",
    "        ax.text(thispeak_height, thispk.h_pulse, '    %.2f m' % thispk.h_pulse, color=thispk.color, weight='bold', va='center')\n",
    "\n",
    "fig.suptitle('example of specular return afterpulses over a melt lake in ICESat-2 data', y=0.98, fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "# figname = 'figs_afterpulses/afterpulse_my-example_lake.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4111824-0b05-40e7-b807-d7c0fddeb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.h[df_join.is_saturated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6916b-965c-4875-ba96-badd166cefcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a619d-31be-47d9-b58f-3623825b08a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36d4e5-6589-46ee-b8ca-8804e4175c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e816708-826f-42fc-b040-30a85c40b1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2790bc5-2790-49b2-bcf7-aeb6723e5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_group = dfs.groupby('pulseid')\n",
    "df_pulse = pulse_group[['xatc', 'lat', 'lon']].mean()\n",
    "df_pulse['ph_count'] = pulse_group['h'].count()\n",
    "df_pulse['ph_count_smooth'] = np.array(df_pulse.ph_count.rolling(smooth_pulse,center=True,min_periods=1).mean())\n",
    "\n",
    "pulse_group_top = df_top.groupby('pulseid')\n",
    "df_pulse_top = pulse_group_top[['xatc', 'lat', 'lon']].mean()\n",
    "df_pulse_top['ph_count'] = pulse_group_top['h'].count()\n",
    "df_pulse_top['ph_count_smooth'] = np.array(df_pulse_top.ph_count.rolling(smooth_pulse,center=True,min_periods=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b451eba-2260-43a9-b558-adf19c982aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578a6ec-b931-4981-8916-cec6a9720d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ff335-ed53-48f6-890e-3da683573ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe_group = df.groupby('mframe')\n",
    "    df_mframe = mframe_group[['lat','lon', 'xatc', 'dt']].mean()\n",
    "    df_mframe.drop(df_mframe.head(1).index,inplace=True)\n",
    "    df_mframe.drop(df_mframe.tail(1).index,inplace=True)\n",
    "    df_mframe['time'] = df_mframe['dt'].map(convert_time_to_string)\n",
    "    df_mframe['xatc_min'] = mframe_group['xatc'].min()\n",
    "    df_mframe['xatc_max'] = mframe_group['xatc'].max()\n",
    "    df_mframe['n_phot'] = mframe_group['h'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf261a8-f5f9-473a-8457-4557fd5b7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulse_dfs[0].loc[pulse_dfs[0].iloc[:5].index]\n",
    "df_pulses = pulse_dfs[0]\n",
    "idx1 = df_pulses[df_pulses.ph_count > 0.99].index\n",
    "idx2 = df_pulses[df_pulses.ph_count > 0.99].index\n",
    "idx1.append(idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28d14d-6478-4e44-96d1-081eb5f74baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c10a-9a7b-445c-ab40-5f7a5c7f3695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070abf89-6b10-48c1-938e-e1076d28382e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa7c1c-75c2-486b-986e-ebbd1f361f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk['photon_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d60701-4ea9-4356-abd1-0e190ec4e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_xlim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce005d-ca3b-48d4-bf97-ac7b6ec4068f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc42a2-ed04-4769-b154-1c9a156619cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5aecf-4953-4a47-ac5c-553dae4a0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417f79b-a646-4834-ad40-2d3941478071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeicelakes-env",
   "language": "python",
   "name": "eeicelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
