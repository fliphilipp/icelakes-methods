{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914b4d0c-cfb9-4fae-aec0-e32f52f7bc49",
   "metadata": {},
   "source": [
    "# Compile lake info/stats into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd41a3ff-14b9-44ca-bca6-1cd5076ba524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "os.environ[\"GDAL_DATA\"] = \"/home/parndt/anaconda3/envs/geo_py37/share/gdal\"\n",
    "os.environ[\"PROJ_LIB\"] = \"/home/parndt/anaconda3/envs/geo_py37/share/proj\"\n",
    "import h5py\n",
    "import math\n",
    "import datetime\n",
    "import traceback\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import timezone\n",
    "from matplotlib.patches import Rectangle\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from icelakes.utilities import convert_time_to_string\n",
    "from IPython.display import Image, display\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from lakeanalysis.utils import dictobj, get_quality_summary, convert_time_to_string, read_melt_lake_h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4670b07-f297-473d-a639-9391a66d1097",
   "metadata": {},
   "source": [
    "# Specify the directory where the data lives\n",
    "Make sure this is the right one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0b495e-db12-48a2-a499-9e4d40b1c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = '/Volumes/nox/Philipp/IceLakesRun1/GlacierLakeDetectionICESat2/'\n",
    "# base_dir = '/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/'\n",
    "base_dir = 'data/lakes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bb4872-a68c-4849-b401-65de40825f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_data_dir = 'detection_out_data/'\n",
    "# out_plot_dir = 'detection_out_data/'\n",
    "# granule_stats_fn = '/Users/parndt/jupyterprojects/IceLakesRun2/stats_combined_GLD2_except91granules.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6672fa3f-dbea-4983-ac3f-6d55df56ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_column_names=['region', 'granule', 'xatc_total', 'xatc_lakes', 'nphot_total', 'nphot_lakes']\n",
    "# df_granule_stats = pd.read_csv(granule_stats_fn, header=None, names=stats_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcd888f-84fb-4482-81de-75e4ebf079c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_xatc = df_granule_stats.xatc_total.sum()\n",
    "# lakes_xatc = df_granule_stats.xatc_lakes.sum()\n",
    "# total_photons = df_granule_stats.nphot_total.sum()\n",
    "# lakes_photons = df_granule_stats.nphot_lakes.sum()\n",
    "# print('Total along-track distance analyzed: %.1f million km (%.1f million miles)' % (total_xatc/1e9, total_xatc/1e9*0.621371))\n",
    "# print('Total along-track distance of lakes: %.1f thousand km (%.1f thousand miles)' % (lakes_xatc/1e6, lakes_xatc/1e6*0.621371))\n",
    "# print('Percentage that is lakes:            %.5f' % (lakes_xatc/total_xatc*100))\n",
    "# print('')\n",
    "# print('Total number of photons analyzed:    %i billion' % (total_photons/1e9))\n",
    "# print('Total number of lake photons:        %i million' % (lakes_photons/1e6))\n",
    "# print('Percentage that is lakes:            %.5f' % (lakes_photons/total_photons*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d1c9-4ef6-40e1-949c-008798ce9cf6",
   "metadata": {},
   "source": [
    "# get lake info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e765e6-c9f4-4fc3-9047-638224ccf011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1248 data files.\n",
      "There are 456 files with zero quality.\n"
     ]
    }
   ],
   "source": [
    "searchfor = '.h5'\n",
    "searchdir = base_dir # + out_data_dir\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist.sort()\n",
    "print('There are %i data files.' % len(filelist))\n",
    "zerolakes = [f for f in filelist if 'lake_10000000_' in f]\n",
    "print('There are %i files with zero quality.' % len(zerolakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9887fc0-056e-4ee7-b362-e3b8f864f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1248 plot files.\n"
     ]
    }
   ],
   "source": [
    "searchfor_img = '.jpg'\n",
    "searchdir = base_dir # + out_plot_dir\n",
    "filelist_plot = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor_img in f)]\n",
    "filelist_plot.sort()\n",
    "print('There are %i plot files.' % len(filelist_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624e9dc3-cf4d-477e-8d9a-49d40e771aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import Point\n",
    "# fn_basins = '../GlacierLakeDetectionICESat2/basins/shapefiles/ANT_basins_merged.shp'\n",
    "# gdf = gpd.read_file(fn_basins).set_index('Subregions')\n",
    "# ep_f = gdf.loc['Ep-F'].geometry\n",
    "# ep_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad415b-99c1-41a5-bef1-03672cda5391",
   "metadata": {},
   "source": [
    "# Run this to compile lake stats by reading in all the available data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a165ce5-88bb-4c83-a837-13a73e268702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lake_mean_delta_time = np.median(f['mframe_data']['dt'][()])\n",
    "# # ATLAS SDP epoch is 2018-01-01:T00.00.00.000000 UTC, from ATL03 data dictionary \n",
    "# ATLAS_SDP_epoch_datetime = datetime(2018, 1, 1, tzinfo=timezone.utc)\n",
    "# ATLAS_SDP_epoch_timestamp = datetime.timestamp(ATLAS_SDP_epoch_datetime)\n",
    "# lake_mean_timestamp = ATLAS_SDP_epoch_timestamp + lake_mean_delta_time\n",
    "# lake_mean_datetime = datetime.fromtimestamp(lake_mean_timestamp, tz=timezone.utc)\n",
    "# time_format_out = '%Y-%m-%dT%H:%M:%SZ'\n",
    "# is2time = datetime.strftime(lake_mean_datetime, time_format_out)\n",
    "# print(is2time)\n",
    "# epoch = lake_mean_delta_time + datetime.timestamp(datetime(2018,1,1))\n",
    "# print(datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%dT%H:%M:%SZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a1681a-0510-41a4-8928-3005bfed7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file  1248 /  1248\n",
      "Number of lakes with missing data: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ice_sheet</th>\n",
       "      <th>melt_season</th>\n",
       "      <th>basin</th>\n",
       "      <th>quality_summary</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>length_water_surfaces</th>\n",
       "      <th>surface_elevation</th>\n",
       "      <th>n_photons_where_water</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>gtx</th>\n",
       "      <th>beam_strength</th>\n",
       "      <th>beam_number</th>\n",
       "      <th>detection_quality</th>\n",
       "      <th>lake_quality</th>\n",
       "      <th>granule_id</th>\n",
       "      <th>lake_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>main_region</th>\n",
       "      <th>basin_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2019</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>69.887407</td>\n",
       "      <td>9.643890</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1465.033137</td>\n",
       "      <td>10215</td>\n",
       "      <td>-47.768970</td>\n",
       "      <td>69.325233</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3l</td>\n",
       "      <td>strong</td>\n",
       "      <td>5</td>\n",
       "      <td>0.906434</td>\n",
       "      <td>77.000666</td>\n",
       "      <td>ATL03_20190810040312_06580403_006_02.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20190810040312_06...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2019</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>64.113204</td>\n",
       "      <td>10.971291</td>\n",
       "      <td>1166.0</td>\n",
       "      <td>1140.253463</td>\n",
       "      <td>4982</td>\n",
       "      <td>-49.035769</td>\n",
       "      <td>68.449787</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3l</td>\n",
       "      <td>strong</td>\n",
       "      <td>5</td>\n",
       "      <td>0.819832</td>\n",
       "      <td>78.101916</td>\n",
       "      <td>ATL03_20190818034635_07800403_006_02.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20190818034635_07...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2019</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>58.072412</td>\n",
       "      <td>12.271815</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>1185.288296</td>\n",
       "      <td>4292</td>\n",
       "      <td>-48.431568</td>\n",
       "      <td>68.981906</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3l</td>\n",
       "      <td>strong</td>\n",
       "      <td>5</td>\n",
       "      <td>0.638567</td>\n",
       "      <td>90.840330</td>\n",
       "      <td>ATL03_20190814035453_07190403_006_02.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20190814035453_07...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2019</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>55.236413</td>\n",
       "      <td>10.936301</td>\n",
       "      <td>998.0</td>\n",
       "      <td>1261.007606</td>\n",
       "      <td>5008</td>\n",
       "      <td>-49.024466</td>\n",
       "      <td>69.738558</td>\n",
       "      <td>...</td>\n",
       "      <td>gt1l</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857479</td>\n",
       "      <td>64.316455</td>\n",
       "      <td>ATL03_20190716051841_02770403_006_02.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20190716051841_02...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2019</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>41.035655</td>\n",
       "      <td>8.154317</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>1465.022620</td>\n",
       "      <td>2144</td>\n",
       "      <td>-47.766681</td>\n",
       "      <td>69.325053</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3r</td>\n",
       "      <td>weak</td>\n",
       "      <td>6</td>\n",
       "      <td>0.628573</td>\n",
       "      <td>65.182764</td>\n",
       "      <td>ATL03_20190810040312_06580403_006_02.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20190810040312_06...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.283780</td>\n",
       "      <td>766.0</td>\n",
       "      <td>1039.573456</td>\n",
       "      <td>9155</td>\n",
       "      <td>-49.706671</td>\n",
       "      <td>70.397772</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3r</td>\n",
       "      <td>weak</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ATL03_20200815102547_07800803_006_01.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20200815102547_07...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.229138</td>\n",
       "      <td>513.0</td>\n",
       "      <td>1271.195157</td>\n",
       "      <td>4435</td>\n",
       "      <td>-50.537348</td>\n",
       "      <td>71.303189</td>\n",
       "      <td>...</td>\n",
       "      <td>gt2r</td>\n",
       "      <td>weak</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079041</td>\n",
       "      <td>ATL03_20200819230220_08490805_006_01.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20200819230220_08...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.228673</td>\n",
       "      <td>390.0</td>\n",
       "      <td>1160.537938</td>\n",
       "      <td>5307</td>\n",
       "      <td>-50.674547</td>\n",
       "      <td>71.181567</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3l</td>\n",
       "      <td>strong</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ATL03_20200819230220_08490805_006_01.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20200819230220_08...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.081306</td>\n",
       "      <td>215.0</td>\n",
       "      <td>1173.193297</td>\n",
       "      <td>2046</td>\n",
       "      <td>-50.621156</td>\n",
       "      <td>71.321467</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3r</td>\n",
       "      <td>weak</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ATL03_20200819230220_08490805_006_01.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20200819230220_08...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>GrIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>GRE_2000_CW</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.144271</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1160.624420</td>\n",
       "      <td>3170</td>\n",
       "      <td>-50.676920</td>\n",
       "      <td>71.181972</td>\n",
       "      <td>...</td>\n",
       "      <td>gt3r</td>\n",
       "      <td>weak</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ATL03_20200819230220_08490805_006_01.h5</td>\n",
       "      <td>simplified_GRE_2000_CW_ATL03_20200819230220_08...</td>\n",
       "      <td>/Users/parndt/jupyterprojects/icelakes-methods...</td>\n",
       "      <td>None</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ice_sheet melt_season        basin  quality_summary  max_depth  \\\n",
       "0         GrIS        2019  GRE_2000_CW        69.887407   9.643890   \n",
       "1         GrIS        2019  GRE_2000_CW        64.113204  10.971291   \n",
       "2         GrIS        2019  GRE_2000_CW        58.072412  12.271815   \n",
       "3         GrIS        2019  GRE_2000_CW        55.236413  10.936301   \n",
       "4         GrIS        2019  GRE_2000_CW        41.035655   8.154317   \n",
       "...        ...         ...          ...              ...        ...   \n",
       "1243      GrIS        2020  GRE_2000_CW         0.000000   9.283780   \n",
       "1244      GrIS        2020  GRE_2000_CW         0.000002   3.229138   \n",
       "1245      GrIS        2020  GRE_2000_CW         0.000000   1.228673   \n",
       "1246      GrIS        2020  GRE_2000_CW         0.000000   2.081306   \n",
       "1247      GrIS        2020  GRE_2000_CW         0.000000   1.144271   \n",
       "\n",
       "      length_water_surfaces  surface_elevation  n_photons_where_water  \\\n",
       "0                    1976.0        1465.033137                  10215   \n",
       "1                    1166.0        1140.253463                   4982   \n",
       "2                    1562.0        1185.288296                   4292   \n",
       "3                     998.0        1261.007606                   5008   \n",
       "4                    1673.0        1465.022620                   2144   \n",
       "...                     ...                ...                    ...   \n",
       "1243                  766.0        1039.573456                   9155   \n",
       "1244                  513.0        1271.195157                   4435   \n",
       "1245                  390.0        1160.537938                   5307   \n",
       "1246                  215.0        1173.193297                   2046   \n",
       "1247                  370.0        1160.624420                   3170   \n",
       "\n",
       "            lon        lat  ...   gtx  beam_strength  beam_number  \\\n",
       "0    -47.768970  69.325233  ...  gt3l         strong            5   \n",
       "1    -49.035769  68.449787  ...  gt3l         strong            5   \n",
       "2    -48.431568  68.981906  ...  gt3l         strong            5   \n",
       "3    -49.024466  69.738558  ...  gt1l         strong            1   \n",
       "4    -47.766681  69.325053  ...  gt3r           weak            6   \n",
       "...         ...        ...  ...   ...            ...          ...   \n",
       "1243 -49.706671  70.397772  ...  gt3r           weak            6   \n",
       "1244 -50.537348  71.303189  ...  gt2r           weak            4   \n",
       "1245 -50.674547  71.181567  ...  gt3l         strong            5   \n",
       "1246 -50.621156  71.321467  ...  gt3r           weak            6   \n",
       "1247 -50.676920  71.181972  ...  gt3r           weak            6   \n",
       "\n",
       "      detection_quality  lake_quality  \\\n",
       "0              0.906434     77.000666   \n",
       "1              0.819832     78.101916   \n",
       "2              0.638567     90.840330   \n",
       "3              0.857479     64.316455   \n",
       "4              0.628573     65.182764   \n",
       "...                 ...           ...   \n",
       "1243           0.000000      0.000000   \n",
       "1244           0.000000      0.079041   \n",
       "1245           0.000000      0.000000   \n",
       "1246           0.000000      0.000000   \n",
       "1247           0.000000      0.000000   \n",
       "\n",
       "                                   granule_id  \\\n",
       "0     ATL03_20190810040312_06580403_006_02.h5   \n",
       "1     ATL03_20190818034635_07800403_006_02.h5   \n",
       "2     ATL03_20190814035453_07190403_006_02.h5   \n",
       "3     ATL03_20190716051841_02770403_006_02.h5   \n",
       "4     ATL03_20190810040312_06580403_006_02.h5   \n",
       "...                                       ...   \n",
       "1243  ATL03_20200815102547_07800803_006_01.h5   \n",
       "1244  ATL03_20200819230220_08490805_006_01.h5   \n",
       "1245  ATL03_20200819230220_08490805_006_01.h5   \n",
       "1246  ATL03_20200819230220_08490805_006_01.h5   \n",
       "1247  ATL03_20200819230220_08490805_006_01.h5   \n",
       "\n",
       "                                                lake_id  \\\n",
       "0     simplified_GRE_2000_CW_ATL03_20190810040312_06...   \n",
       "1     simplified_GRE_2000_CW_ATL03_20190818034635_07...   \n",
       "2     simplified_GRE_2000_CW_ATL03_20190814035453_07...   \n",
       "3     simplified_GRE_2000_CW_ATL03_20190716051841_02...   \n",
       "4     simplified_GRE_2000_CW_ATL03_20190810040312_06...   \n",
       "...                                                 ...   \n",
       "1243  simplified_GRE_2000_CW_ATL03_20200815102547_07...   \n",
       "1244  simplified_GRE_2000_CW_ATL03_20200819230220_08...   \n",
       "1245  simplified_GRE_2000_CW_ATL03_20200819230220_08...   \n",
       "1246  simplified_GRE_2000_CW_ATL03_20200819230220_08...   \n",
       "1247  simplified_GRE_2000_CW_ATL03_20200819230220_08...   \n",
       "\n",
       "                                              file_name main_region  \\\n",
       "0     /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "1     /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "2     /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "3     /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "4     /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "...                                                 ...         ...   \n",
       "1243  /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "1244  /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "1245  /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "1246  /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "1247  /Users/parndt/jupyterprojects/icelakes-methods...        None   \n",
       "\n",
       "      basin_name  \n",
       "0             CW  \n",
       "1             CW  \n",
       "2             CW  \n",
       "3             CW  \n",
       "4             CW  \n",
       "...          ...  \n",
       "1243          CW  \n",
       "1244          CW  \n",
       "1245          CW  \n",
       "1246          CW  \n",
       "1247          CW  \n",
       "\n",
       "[1248 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_missing_data = 0\n",
    "\n",
    "for i,fn in enumerate(filelist):\n",
    "    print('reading file %5i / %5i' % (i+1, len(filelist)), end='\\r')\n",
    "\n",
    "    try:\n",
    "        with h5py.File(fn, 'r+') as f:\n",
    "            ice_sheet = f['properties']['ice_sheet'][()].decode('utf-8')\n",
    "            melt_season = f['properties']['melt_season'][()].decode('utf-8')\n",
    "            polygon_name = f['properties']['polygon_name'][()].decode('utf-8')\n",
    "            max_depth = f['properties']['max_depth'][()]\n",
    "            length_water_surfaces = f['properties']['length_water_surfaces'][()]\n",
    "            surface_elevation = f['properties']['surface_elevation'][()]\n",
    "            n_photons_where_water = f['properties']['n_photons_where_water'][()]\n",
    "            lon = f['properties']['lon'][()]\n",
    "            lat = f['properties']['lat'][()]\n",
    "            lon_min = f['properties']['lon_min'][()]\n",
    "            lon_max = f['properties']['lon_max'][()]\n",
    "            lat_min = f['properties']['lat_min'][()]\n",
    "            lat_max = f['properties']['lat_max'][()]\n",
    "            cycle_number = f['properties']['cycle_number'][()]\n",
    "            rgt = f['properties']['rgt'][()]\n",
    "            gtx = f['properties']['gtx'][()].decode('utf-8')\n",
    "            beam_strength = f['properties']['beam_strength'][()].decode('utf-8')\n",
    "            beam_number = f['properties']['beam_number'][()]\n",
    "            granule_id = f['properties']['granule_id'][()].decode('utf-8')\n",
    "            lake_id = f['properties']['lake_id'][()].decode('utf-8')\n",
    "\n",
    "            # this is a fix. previously times were not UTC\n",
    "            date_time = convert_time_to_string(np.median(f['mframe_data']['dt'][()]))\n",
    "            if 'time_utc' in f['properties'].keys():\n",
    "                del f['properties/time_utc']\n",
    "            dset = f.create_dataset('properties/time_utc', data=date_time)\n",
    "            \n",
    "            lake_quality = f['properties']['lake_quality'][()]\n",
    "            detection_quality = f['properties']['detection_quality'][()]\n",
    "            quality_summary = f['properties']['quality_summary'][()]\n",
    "            \n",
    "            #############################################\n",
    "            # if 'test_filchner' in polygon_name:\n",
    "            #     polygon_name = 'geojsons/simplified_ANT_1000_West_J-Jpp.geojson'\n",
    "            #     del f['properties/polygon_name']\n",
    "            #     dset = f.create_dataset('properties/polygon_name', data=polygon_name)\n",
    "\n",
    "            # if 'test_ross' in polygon_name:\n",
    "            #     polygon_name = 'geojsons/simplified_ANT_1000_East_E-Ep.geojson'\n",
    "            #     del f['properties/polygon_name']\n",
    "            #     dset = f.create_dataset('properties/polygon_name', data=polygon_name)\n",
    "\n",
    "            # if ('_E-Ep' in polygon_name) or ('_Ep-F' in polygon_name):\n",
    "            #     pt = gpd.GeoDataFrame(geometry=[Point(lon, lat)], crs='EPSG:4326').to_crs(gdf.crs).loc[0].geometry\n",
    "            #     if pt.within(ep_f):\n",
    "            #         polygon_name = 'geojsons/simplified_ANT_1000_West_Ep-F.geojson'\n",
    "            #         print(polygon_name)\n",
    "            #     else:\n",
    "            #         polygon_name = 'geojsons/simplified_ANT_1000_East_E-Ep.geojson'\n",
    "            #     del f['properties/polygon_name']\n",
    "            #     dset = f.create_dataset('properties/polygon_name', data=polygon_name)\n",
    "            #############################################\n",
    "        \n",
    "        basin = polygon_name.replace('simplified_', '')\n",
    "        file_name = fn\n",
    "    \n",
    "        datadict = {\n",
    "            'ice_sheet': ice_sheet,\n",
    "            'melt_season': melt_season,\n",
    "            'basin': basin,\n",
    "            'quality_summary': quality_summary,\n",
    "            'max_depth': max_depth,\n",
    "            'length_water_surfaces': length_water_surfaces,\n",
    "            'surface_elevation': surface_elevation,\n",
    "            'n_photons_where_water': n_photons_where_water,\n",
    "            'lon': lon,\n",
    "            'lat': lat,\n",
    "            'date_time': date_time,\n",
    "            'lon_min': lon_min,\n",
    "            'lon_max': lon_max,\n",
    "            'lat_min': lat_min,\n",
    "            'lat_max': lat_max,\n",
    "            'cycle_number': cycle_number,\n",
    "            'rgt': rgt,\n",
    "            'gtx': gtx,\n",
    "            'beam_strength': beam_strength,\n",
    "            'beam_number': beam_number,\n",
    "            'detection_quality': detection_quality,\n",
    "            'lake_quality': lake_quality,\n",
    "            'granule_id': granule_id,\n",
    "            'lake_id': lake_id,\n",
    "            'file_name': file_name\n",
    "        }\n",
    "    \n",
    "        if i == 0: \n",
    "            df = pd.DataFrame(datadict, index=[0])\n",
    "        else:\n",
    "            df.loc[i] = datadict.values()\n",
    "\n",
    "    except:\n",
    "        num_missing_data += 1\n",
    "        traceback.print_exc()\n",
    "\n",
    "print('\\nNumber of lakes with missing data: %i' % num_missing_data)\n",
    "\n",
    "df['main_region'] = df.apply(lambda x: 'AIS_'+x.basin.split('_')[2] if x.ice_sheet == 'AIS' else None, axis=1)\n",
    "df['basin_name'] = df.apply(lambda x: x.basin.split('_')[-1], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3eaccf-be57-4438-8f24-67ce8c2304e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.basin=='test_ross','basin'] = 'geojsons/simplified_ANT_1000_East_E-Ep.geojson'\n",
    "# df.loc[df.basin=='test_ross','lake_id'] = df.loc[df.basin=='test_ross','lake_id'].replace('test_ross','simplified_ANT_1000_East_E-Ep')\n",
    "# df['main_region'] = df.apply(lambda x: 'AIS_'+x.basin.split('_')[2] if x.ice_sheet == 'AIS' else None, axis=1)\n",
    "# df['basin_name'] = df.apply(lambda x: x.basin.split('_')[-1], axis=1)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84dbed1-d72b-49eb-b57e-80e6641427dc",
   "metadata": {},
   "source": [
    "# Save the lake info to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d2b82e-0053-47b0-88dc-5391ddf7819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/lakestats_methods_paper.csv\n"
     ]
    }
   ],
   "source": [
    "out_file_csv = 'data/lakestats_methods_paper.csv'\n",
    "\n",
    "out_path_csv = out_file_csv\n",
    "df.to_csv(out_path_csv, index=False)\n",
    "print(out_path_csv)\n",
    "# '/Volumes/nox/Philipp/IceLakesRun1/GlacierLakeDetectionICESat2/GLD1_all_lakes.csv'\n",
    "# '/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_all_returned_lakes.csv'\n",
    "# '/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_lakestats_for-wais.csv'\n",
    "# /Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_lakestats_for-wais_ross-fixed.csv\n",
    "# /Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_lakestats_for-wais_ross-fixed_all.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab5dcb-ab93-48a8-a543-d75f9c5a47ad",
   "metadata": {},
   "source": [
    "# Check that it worked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adac4a98-9a0f-4bf2-ad9a-7fdb75b3c53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ANT_1000_East_B-C', 'GRE_2000_CW'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(out_path_csv)\n",
    "# df['basin'] = df.apply(lambda x: x.basin.replace('geojsons/', '').replace('.geojson',''), axis=1)\n",
    "np.unique(df.basin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bf3ae-907a-41e6-9bf4-af1342fb498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854972c7-ac03-4ec6-914a-edd338066647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b61d3f-ef5a-426c-a170-6f55c8565e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322d420-39bb-4eec-90b1-3a8d46bb87b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe73cd-7356-44f6-be4f-ee1da82d7af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd1319-66de-4444-a56b-f35407ae58d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83c2e686-28ef-4783-bc7d-824c1dbb9b6f",
   "metadata": {},
   "source": [
    "# Subset for good lakes only for WAIS Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbd92f-e6d9-46e1-ae11-39c1a0f3dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\n",
    "    'lake_09999770_AIS_2020-21_simplified_ANT_1000_West_Ep-F_ATL03_20210319070241_13021012_006_01_gt1l_0000.h5',\n",
    "    'lake_09999783_AIS_2019-20_simplified_ANT_1000_East_K-A_ATL03_20191104231510_05960512_006_01_gt3l_0018.h5',\n",
    "    'lake_09999834_AIS_2019-20_simplified_ANT_1000_Peninsula_I-Ipp_ATL03_20200222201311_08870612_006_01_gt2r_0000.h5',\n",
    "    'lake_09994291_AIS_2020-21_simplified_ANT_1000_West_F-G_ATL03_20201125223347_09580910_006_01_gt2l_0000.h5',\n",
    "    'lake_09999865_AIS_2021-22_simplified_ANT_1000_West_H-Hp_ATL03_20211229140312_01111412_006_01_gt2l_0000.h5',\n",
    "]\n",
    "\n",
    "wais_list = 'lakelistgood_temp20230922.csv'\n",
    "df_sel = pd.read_csv(wais_list,header=None,names=['filename'])\n",
    "df_sel.iloc[0].filename\n",
    "df_sel['qual_int'] = df_sel.apply(lambda x: int(x.filename.split('_')[1]), axis=1)\n",
    "df_sel['is_data'] = df_sel.apply(lambda x: '.h5' in x.filename, axis=1)\n",
    "df_sel = df_sel[df_sel.is_data & (df_sel.qual_int != 10000000)].reset_index(drop=True)\n",
    "df['fn_only'] = df.apply(lambda x: x.file_name.split('/')[-1], axis=1)\n",
    "filelist_good = list(df_sel.filename)\n",
    "df['is_good_wais'] = df.apply(lambda x: (x.fn_only in filelist_good) and (x.fn_only not in to_remove), axis=1)\n",
    "df_forwais = df[df.is_good_wais]\n",
    "df_forwais['basin']= df_forwais.apply(lambda x: x.basin.replace('geojsons/', '').replace('.geojson', ''), axis=1)\n",
    "df_forwais['basin_name']= df_forwais.apply(lambda x: x.basin.replace('geojsons/', '').replace('.geojson', ''), axis=1)\n",
    "df_forwais\n",
    "\n",
    "waislist_out = '/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_lakestats_for-wais_ross-fix_gooddata.csv'\n",
    "df_forwais.to_csv(waislist_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938e47-5aed-4804-b0bd-e3178f6e7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_lakestats_for-wais_ross-fix_gooddata.csv')\n",
    "df_ant = df[df.ice_sheet=='AIS']\n",
    "df.groupby('basin_name')['lake_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b7722-8f5b-4115-873a-9e357a147364",
   "metadata": {},
   "source": [
    "# now get granule stats from the actual selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57fed8-f52e-4ca9-99b2-43f4e4be09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_xatc = df_granule_stats.xatc_total.sum()\n",
    "lakes_xatc = df.length_water_surfaces.sum()\n",
    "total_photons = df_granule_stats.nphot_total.sum()\n",
    "lakes_photons = df.n_photons_where_water.sum()\n",
    "print('Total along-track distance analyzed:  %.1f million km (%.1f million miles)' % (total_xatc/1e9, total_xatc/1e9*0.621371))\n",
    "print('Total along-track distance of lakes:  %.1f thousand km (%.1f thousand miles)' % (lakes_xatc/1e6, lakes_xatc/1e6*0.621371))\n",
    "print('')\n",
    "print('Total number of photons analyzed:     %i billion' % (total_photons/1e9))\n",
    "print('Total number of lake photons:         %i million' % (lakes_photons/1e6))\n",
    "print('')\n",
    "print('Along-track distance lake percentage: %.3f %%' % (lakes_xatc/total_xatc*100))\n",
    "print('Photon count percentage:              %.3f %%' % (lakes_photons/total_photons*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1316e-ef8c-4066-9ecc-905a49da0ac1",
   "metadata": {},
   "source": [
    "### This shows that lake segements have a lower photon density than non-lake ones. \n",
    "This at first seems counterintuitive, but lakes are harder to detect when there is more background noise..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4cdaa-d747-45b6-80c8-07f961c534b5",
   "metadata": {},
   "source": [
    "# Now get the stats for all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d11207-6c4c-4d59-8cda-1e463b2f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_region'] = df.apply(lambda x: 'AIS_'+x.basin.split('_')[2] if x.ice_sheet == 'AIS' else None, axis=1)\n",
    "df['basin_name'] = df.apply(lambda x: x.basin.split('_')[-1], axis=1)\n",
    "df_granule_stats['ice_sheet'] = df_granule_stats.apply(lambda x: 'AIS' if x.region[:3] == 'ANT' else 'GrIS', axis=1)\n",
    "df_granule_stats['main_region'] = df_granule_stats.apply(lambda x: 'AIS_'+x.region.split('_')[2] if x.ice_sheet == 'AIS' else None, axis=1)\n",
    "df_granule_stats['basin_name'] = df_granule_stats.apply(lambda x: x.region.split('_')[3][:-8] if x.ice_sheet == 'AIS' else x.region.split('_')[2][:-8], axis=1)\n",
    "\n",
    "def get_counts_by_region(df_g, df_l, group='ice_sheet'):\n",
    "    if group=='all':\n",
    "        granule_stats = pd.DataFrame(df_g[['xatc_total', 'nphot_total']].sum().to_dict(), index=['Total'])\n",
    "        lake_stats = pd.DataFrame(df_l[['length_water_surfaces', 'n_photons_where_water']].sum().to_dict(), index=['Total'])\n",
    "    else:\n",
    "        granule_stats = df_g.groupby(by=group)[['xatc_total', 'nphot_total']].sum()\n",
    "        lake_stats = df_l.groupby(by=group)[['length_water_surfaces', 'n_photons_where_water']].sum()\n",
    "    stats_merge = pd.concat((granule_stats, lake_stats), axis=1)\n",
    "    stats_merge['percentage_xatc'] = stats_merge.length_water_surfaces/stats_merge.xatc_total*100\n",
    "    stats_merge['percentage_photons'] = stats_merge.n_photons_where_water/stats_merge.nphot_total*100\n",
    "    return stats_merge\n",
    "\n",
    "dfs_stat = []\n",
    "for grp in ['all', 'ice_sheet', 'main_region', 'basin_name']:\n",
    "    dfs_stat.append(get_counts_by_region(df_granule_stats, df, group=grp))\n",
    "df_stats = pd.concat(dfs_stat)\n",
    "df_stats.to_csv('/Volumes/nox/Philipp/IceLakesRun2/GlacierLakeDetectionICESat2/GLD2_granulestats_for-wais_ross-fix_gooddata.csv')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69b225-2e99-4508-afc3-6a1a3dc832fa",
   "metadata": {},
   "source": [
    "# Code below for random plotting to check things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7eb46-3fa3-4aeb-9c9a-9754f16afe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsort = df.sort_values(by='quality_summary', ascending=False)\n",
    "df_ant = dfsort[dfsort.ice_sheet=='AIS'].reset_index(drop=True)\n",
    "df_gre = dfsort[dfsort.ice_sheet=='GrIS'].reset_index(drop=True)\n",
    "# df_ant['papergranule'] = df_ant.apply(lambda x: 'ATL03_20190102184312_00810210_' in x.granule_id ,axis=1)\n",
    "# df_ant = df_ant[df_ant.papergranule & (df_ant.gtx=='gt2l')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccff4c-1920-4642-9b4c-91f85dc0af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fn = df_ant.loc[i].file_name\n",
    "    fig, ax = plt.subplots(figsize=[8,5])\n",
    "    with h5py.File(fn, 'r') as f:\n",
    "        xatc = f['photon_data']['xatc'][()]\n",
    "        h = f['photon_data']['h'][()]\n",
    "    ax.scatter(xatc, h, s=1, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe4eaa-2ca4-4339-af2b-bd5a754de105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2665b-0617-4faf-ba5d-c3faf78b65a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee7456-3c30-4775-a8ea-628709a6392f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3251e6-b55d-4237-8a83-1bec4f0b53b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7a160-f04e-4d80-a447-e0f9c45f029b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da701225-cbf2-4c54-8f57-63c78071a11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa3984-e61a-4576-8f61-ba8fd63bbe63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf086204-ffd5-4898-98d9-58f1cef460ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4c42b-6832-4a10-8812-e6da865346b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0829a7-aa26-4b08-b823-cfdb1e7f1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakes = df[((df.quality_summary > 0) \n",
    "               & (df.surface_elevation > 1.0) \n",
    "               & (df.max_depth < 50) \n",
    "               & (df.max_depth > 0.5))].copy()\n",
    "# df_lakes = df[df.quality_summary > 0]\n",
    "df_lakes.sort_values(by='quality_summary', ascending=False, inplace=True)\n",
    "len(df_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d36821-1526-40f8-ab20-9ff79496acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_sup = 14\n",
    "sz_tit_l = 12\n",
    "sz_tit_s = 7\n",
    "sz_lab_l = 10\n",
    "sz_lab_s = 6\n",
    "sz_tck_l = 8\n",
    "sz_tck_s = 5\n",
    "colors = {'GrIS': '#D00C33', # Greenland flag\n",
    "          'AIS': '#3A7DCE' # Antarctica, Graham Bartram Design\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f5be0-37a9-4cea-8a66-0ab70f3b6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=[8,4], ncols=2)\n",
    "step = 25\n",
    "for i,sheet in enumerate(['AIS', 'GrIS']):\n",
    "    dfi = df_lakes[df_lakes.ice_sheet == sheet]\n",
    "    uplim = 1000 if sheet=='AIS' else 2000\n",
    "    sheet_name = 'Antarctic Ice Sheet' if sheet=='AIS' else 'Greenland Ice Sheet'\n",
    "    bins = np.arange(0,uplim+step,step)\n",
    "    mids = bins[:-1] + np.diff(bins)\n",
    "    hist = np.histogram(dfi.surface_elevation, bins=bins)[0]\n",
    "    ax = axs[i]\n",
    "    ax.bar(mids, hist, np.diff(bins)[0], color=colors[sheet])\n",
    "    ax.set_title(sheet_name, fontsize=sz_tit_l)\n",
    "    if i==0:\n",
    "        ax.set_ylabel('counts of lakes per %i m bin' % step, fontsize=sz_lab_l)\n",
    "    ax.set_xlabel('elevation above geoid (m)', fontsize=sz_lab_l)\n",
    "    ax.set_xlim((0,uplim))\n",
    "    ax.tick_params(axis='both', labelsize=sz_tck_l)\n",
    "\n",
    "fig.suptitle('surface elevation distributions of ICESat-2 lakes', fontsize=sz_sup)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2088423-c7ca-4282-a42f-76ee85b0ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=[6,6], ncols=5, nrows=5)\n",
    "axs = axs.flatten()\n",
    "basins = np.unique(df_lakes.basin)\n",
    "# maybe order basins in a reasonable way??\n",
    "\n",
    "step = 50\n",
    "for i,basin in enumerate(basins):\n",
    "    dfi = df_lakes[df_lakes.basin == basin]\n",
    "    sheet = 'AIS' if basin[:3]=='ANT' else 'GrIS'\n",
    "    parms = basin.split('_')\n",
    "    if sheet == 'AIS':\n",
    "        basin_disp = '%s (%s)' % (parms[3], parms[2])\n",
    "        basin_name = parms[3]\n",
    "    else:\n",
    "        basin_disp = parms[2]\n",
    "        basin_name = basin_disp\n",
    "\n",
    "    uplim = 1000 if sheet=='AIS' else 2000\n",
    "    bins = np.arange(0,uplim+step,step)\n",
    "    mids = bins[:-1] + np.diff(bins)\n",
    "    hist = np.histogram(dfi.surface_elevation, bins=bins)[0]\n",
    "    ax = axs[i]\n",
    "    ax.bar(mids, hist, np.diff(bins)[0], color=colors[sheet])\n",
    "    ax.set_title(basin_disp, fontsize=sz_tit_s)\n",
    "    if i%5==0:\n",
    "        ax.set_ylabel('counts', fontsize=sz_lab_s)\n",
    "    if i>=20:\n",
    "        ax.set_xlabel('elevation (m)', fontsize=sz_lab_s)\n",
    "    ax.set_xlim((0,uplim))\n",
    "    ax.tick_params(axis='both', labelsize=sz_tck_s)\n",
    "    \n",
    "fig.tight_layout(pad=0.2, h_pad=0.5, w_pad=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5024126-a558-4547-8cdc-57ff03f3c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "sz_sup = 12\n",
    "sz_tit_l = 10\n",
    "sz_tit_s = 7\n",
    "sz_lab_l = 8\n",
    "sz_lab_s = 6\n",
    "sz_tck_l = 8\n",
    "sz_tck_s = 5\n",
    "colors = {'GrIS': '#D00C33', # Greenland flag\n",
    "          'AIS': '#3A7DCE' # Antarctica, Graham Bartram Design\n",
    "         }\n",
    "\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "spec = gridspec.GridSpec(ncols=10, nrows=15, figure=fig)\n",
    "axs = []\n",
    "axs.append(fig.add_subplot(spec[:5, :5]))\n",
    "axs.append(fig.add_subplot(spec[:5, 5:]))\n",
    "for i in range(25):\n",
    "    xloc = 5 + int(i/5)*2\n",
    "    yloc = (i%5) * 2\n",
    "    axs.append(fig.add_subplot(spec[xloc:xloc+2, yloc:yloc+2]))\n",
    "\n",
    "step = 25\n",
    "for i,sheet in enumerate(['AIS', 'GrIS']):\n",
    "    dfi = df_lakes[df_lakes.ice_sheet == sheet]\n",
    "    uplim = 1000 if sheet=='AIS' else 2000\n",
    "    sheet_name = 'Antarctic Ice Sheet' if sheet=='AIS' else 'Greenland Ice Sheet'\n",
    "    bins = np.arange(0,uplim+step,step)\n",
    "    mids = bins[:-1] + np.diff(bins)\n",
    "    hist = np.histogram(dfi.surface_elevation, bins=bins)[0]\n",
    "    ax = axs[i]\n",
    "    ax.bar(mids, hist, np.diff(bins)[0], color=colors[sheet])\n",
    "    ax.set_title(sheet_name, fontsize=sz_tit_l)\n",
    "    if i==0:\n",
    "        ax.set_ylabel('counts of lakes per %i m bin' % step, fontsize=sz_lab_l)\n",
    "    ax.set_xlabel('elevation above geoid (m)\\n', fontsize=sz_lab_l)\n",
    "    ax.set_xlim((0,uplim))\n",
    "    ax.tick_params(axis='both', labelsize=sz_tck_l)\n",
    "\n",
    "basins = np.unique(df_lakes.basin)\n",
    "# maybe order basins in a reasonable way??\n",
    "step = 50\n",
    "for i,basin in enumerate(basins):\n",
    "    dfi = df_lakes[df_lakes.basin == basin]\n",
    "    sheet = 'AIS' if basin[:3]=='ANT' else 'GrIS'\n",
    "    parms = basin.split('_')\n",
    "    if sheet == 'AIS':\n",
    "        basin_disp = '%s (%s)' % (parms[3], parms[2])\n",
    "        basin_name = parms[3]\n",
    "    else:\n",
    "        basin_disp = parms[2]\n",
    "        basin_name = basin_disp\n",
    "\n",
    "    uplim = 1000 if sheet=='AIS' else 2000\n",
    "    bins = np.arange(0,uplim+step,step)\n",
    "    mids = bins[:-1] + np.diff(bins)\n",
    "    hist = np.histogram(dfi.surface_elevation, bins=bins)[0]\n",
    "    ax = axs[i+2]\n",
    "    ax.bar(mids, hist, np.diff(bins)[0], color=colors[sheet])\n",
    "    ax.margins(x=0, y=0)\n",
    "    # if i%5==0:\n",
    "    #     ax.set_ylabel('counts', fontsize=sz_lab_s, labelpad=1)\n",
    "    # if i>=20:\n",
    "    #     ax.set_xlabel('elevation', fontsize=sz_lab_s, labelpad=1)\n",
    "    ax.set_xlim((0,uplim))\n",
    "    ax.tick_params(axis='both', labelsize=sz_tck_s)\n",
    "    ax.text(0.5, 1.03, basin_disp, fontsize=sz_tit_s, transform=ax.transAxes, ha='center', va='bottom')\n",
    "\n",
    "fig.suptitle('surface elevation distributions of ICESat-2 lakes', fontsize=sz_sup)\n",
    "fig.tight_layout(h_pad=0.01, w_pad=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d801966-17f5-4041-ba9e-7d23d569a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakes_hq = df_lakes[df_lakes.quality_summary > 0.0]\n",
    "len(df_lakes_hq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af7243-c9a3-40e4-afec-ccea571377f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e9125-cd1d-4013-8304-e27ddd1d22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tckdtms = [datetime.datetime(2019,m,1) for m in np.arange(5,10)]\n",
    "tckdoys = [d.timetuple().tm_yday for d in tckdtms]\n",
    "tckstrs = [datetime.datetime.strftime(d, '%b %d') for d in tckdtms]\n",
    "tckstrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7116fb-cd05-4f24-b1a4-2c0588033c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "def get_doy(x):\n",
    "    doy = x.dtime.timetuple().tm_yday\n",
    "    if calendar.isleap(x.dtime.year):\n",
    "        if doy==60:\n",
    "            doy = -1\n",
    "        if doy>60:\n",
    "            doy -= 1\n",
    "    return doy\n",
    "    \n",
    "fig, axs = plt.subplots(figsize=[8,4], ncols=2)\n",
    "step = 25\n",
    "for i,sheet in enumerate(['AIS', 'GrIS']):\n",
    "    dfi = df_lakes_hq[df_lakes_hq.ice_sheet == sheet]\n",
    "    dfdt = dfi[['ice_sheet', 'basin', 'date_time', 'quality_summary']].copy()\n",
    "    dfdt['dtime'] = dfdt.apply(lambda x: datetime.datetime.strptime(x.date_time,'%Y-%m-%dT%H:%M:%SZ'), axis=1)\n",
    "    dfdt['doy'] = dfdt.apply(get_doy, axis=1)\n",
    "    cnts = dfdt.groupby('doy')['dtime'].count()\n",
    "    if -1 in cnts.index:\n",
    "        cnts = cnts.drop([-1]) # drop feb 29 values\n",
    "    dayhist = pd.DataFrame({'doy': np.array(cnts.index), 'cnts': np.array(cnts)})\n",
    "    dayhist.loc[dayhist.doy>300,'doy'] -= 365\n",
    "    ax = axs[i]\n",
    "    ax.bar(dayhist.doy, dayhist.cnts, 1, color=colors[sheet])\n",
    "    months = [5, 6, 7, 8, 9, 10] if sheet == 'GrIS' else [11, 12, 1, 2, 3, 4]\n",
    "    tckdtms = [datetime.datetime(2019,m,1) for m in months]\n",
    "    tckdoys = [d.timetuple().tm_yday for d in tckdtms]\n",
    "    tckdoys = [d-365 if d>300 else d for d in tckdoys]\n",
    "    tckstrs = [datetime.datetime.strftime(d, '%b %d') for d in tckdtms]\n",
    "    ax.set_xticks(tckdoys)\n",
    "    ax.set_xticklabels(tckstrs)\n",
    "    print(tckdoys, tckstrs)\n",
    "\n",
    "    sheet_name = 'Antarctic Ice Sheet' if sheet=='AIS' else 'Greenland Ice Sheet'\n",
    "    ax.set_title(sheet_name, fontsize=sz_tit_l)\n",
    "    if i==0:\n",
    "        ax.set_ylabel('counts of lakes by day of year', fontsize=sz_lab_l)\n",
    "    ax.set_xlabel('time of year', fontsize=sz_lab_l)\n",
    "    ax.set_xlim((tckdoys[0], tckdoys[-1]))\n",
    "    ax.tick_params(axis='both', labelsize=sz_tck_l)\n",
    "\n",
    "fig.suptitle('time of year for ICESat-2 lakes', fontsize=sz_sup)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f729708-d70f-4984-9a31-ca785084c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_basin = '../GlacierLakeDetectionICESat2/basins/shapefiles/Basins_Antarctica_v02.shp'\n",
    "fn_shelf = '../GlacierLakeDetectionICESat2/basins/shapefiles/IceShelf_Antarctica_v02.shp'\n",
    "fn_coast = '../GlacierLakeDetectionICESat2/basins/shapefiles/Coastline_Antarctica_v02.shp'\n",
    "fn_ground = '../GlacierLakeDetectionICESat2/basins/shapefiles/GroundingLine_Antarctica_v02.shp'\n",
    "fn_merged =  '../GlacierLakeDetectionICESat2/basins/shapefiles/ANT_basins_merged.shp'\n",
    "fn_thresh = '../GlacierLakeDetectionICESat2/basins/shapefiles/ANT_basins_thresh1000.shp'\n",
    "\n",
    "ant_gdf_basin = gpd.read_file(fn_basin)\n",
    "ant_gdf_shelf = gpd.read_file(fn_shelf)\n",
    "ant_gdf_coast = gpd.read_file(fn_coast)\n",
    "ant_gdf_ground = gpd.read_file(fn_ground)\n",
    "ant_gdf_merged = gpd.read_file(fn_merged)\n",
    "ant_gdf_thresh = gpd.read_file(fn_thresh)\n",
    "\n",
    "fn_basin = '../GlacierLakeDetectionICESat2/basins/shapefiles/Greenland_Basins_PS_v1.4.2.shp'\n",
    "fn_merged = '../GlacierLakeDetectionICESat2/basins/shapefiles/GRE_basins_merged.shp'\n",
    "fn_thresh = '../GlacierLakeDetectionICESat2/basins/shapefiles/GRE_basins_thresh2000.shp'\n",
    "gre_gdf_basin = gpd.read_file(fn_basin)\n",
    "gre_gdf_basin['geometry'] = gre_gdf_basin.geometry.buffer(1)\n",
    "gre_gdf_merged = gpd.read_file(fn_merged)\n",
    "gre_gdf_thresh = gpd.read_file(fn_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3689f5-c03a-47e2-b1c3-01b1e2d48962",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lakes = gpd.GeoDataFrame(df_lakes, geometry=gpd.points_from_xy(df_lakes.lon, df_lakes.lat), crs=\"EPSG:4326\")\n",
    "gdf_ant = gdf_lakes[gdf_lakes.ice_sheet=='AIS'].to_crs(ant_gdf_basin.crs)\n",
    "gdf_gre = gdf_lakes[gdf_lakes.ice_sheet=='GrIS'].to_crs(gre_gdf_basin.crs)\n",
    "for gdf in [gdf_ant, gdf_gre]:\n",
    "    gdf['x'] = gdf.apply(lambda x: x.geometry.x, axis=1)\n",
    "    gdf['y'] = gdf.apply(lambda x: x.geometry.y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce802eee-ed11-4e3c-ba2c-76ca079ff1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=[9,5])\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1:])\n",
    "\n",
    "# Antarctica\n",
    "ax = ax2\n",
    "ant_gdf_merged.plot(column='Subregions', cmap=cmc.grayCS, alpha=0.2, ax=ax)\n",
    "ant_gdf_merged.exterior.plot(color='gray', ax=ax, lw=0.1)\n",
    "ant_gdf_shelf.plot(color='blue', alpha=0.1, ax=ax, lw=0.1)\n",
    "ant_gdf_shelf.boundary.plot(color='gray', ax=ax, lw=0.2)\n",
    "ant_gdf_thresh.exterior.plot(color='green', ax=ax, lw=0.1)\n",
    "ant_gdf_basin.dissolve().boundary.plot(color='gray', ax=ax, lw=0.2)\n",
    "gdf_ant.sort_values(by='quality_summary', inplace=True)\n",
    "ax.scatter(gdf_ant.x, gdf_ant.y, s=3, alpha=np.clip(gdf_ant.quality_summary**(1/5),0,1), \n",
    "           c=np.clip(gdf_ant.quality_summary**(1/8),0,1), cmap=cm['hot_r'], vmin=0, vmax=1.66)\n",
    "\n",
    "ax.scatter(0,0,s=3,color='k')\n",
    "def add_shelf_label(shelf, min_area=1e10):\n",
    "    if shelf.geometry.area > min_area: \n",
    "        ax.annotate(text=shelf['NAME'], xy=shelf.geometry.centroid.coords[0], ha='center',va='center',\n",
    "                                       color='b',fontsize=4)\n",
    "ant_gdf_shelf.reset_index().apply(add_shelf_label, axis=1);\n",
    "\n",
    "ant_gdf_merged.apply(lambda x: ax.annotate(text=x['Subregions'], xy=x.geometry.centroid.coords[0], ha='center',va='center',\n",
    "                                       color='k',fontsize=6, weight='bold'),axis=1);\n",
    "ax.set_title('Antarctic Ice Sheet', fontsize=10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Greenland\n",
    "ax = ax1\n",
    "gre_gdf_merged.plot(column='SUBREGION1', cmap=cmc.grayCS, alpha=0.2, ax=ax)\n",
    "gre_gdf_merged.exterior.plot(color='gray', ax=ax, lw=0.1)\n",
    "gre_gdf_thresh.exterior.plot(color='green', ax=ax, lw=0.1)\n",
    "gre_gdf_basin.dissolve().boundary.plot(color='gray', ax=ax, lw=0.2)\n",
    "gdf_gre.sort_values(by='quality_summary', inplace=True)\n",
    "ax.scatter(gdf_gre.x, gdf_gre.y, s=1, alpha=np.clip(gdf_gre.quality_summary**(1/5),0,1), \n",
    "           c=np.clip(gdf_gre.quality_summary**(1/8),0,1), cmap=cm['hot_r'], vmin=0, vmax=1.66)\n",
    "gre_gdf_merged.apply(lambda x: ax.annotate(text=x['SUBREGION1'], xy=x.geometry.centroid.coords[0], ha='center',va='center',\n",
    "                                       color='k',fontsize=6, weight='bold'),axis=1);\n",
    "ax.set_title('Greenland Ice Sheet', fontsize=10)\n",
    "\n",
    "ax.axis('off')\n",
    "fig.suptitle('melt lakes identified in ICESat-2 data', fontsize=14)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea100efd-0b5d-46b8-9efa-2d2e5cffbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thisplot = 'gre'\n",
    "ax = ax2 if thisplot=='ant' else ax1\n",
    "thisgdf = gdf_ant if thisplot=='ant' else gdf_gre\n",
    "xl = ax.get_xlim()\n",
    "yl = ax.get_ylim()\n",
    "thislk = thisgdf[\n",
    "    (thisgdf.x>xl[0]) &\n",
    "    (thisgdf.x<xl[1]) &\n",
    "    (thisgdf.y>yl[0]) &\n",
    "    (thisgdf.y<yl[1])  \n",
    "].copy()\n",
    "thislk.sort_values(by='quality_summary', ascending=False, inplace=True)\n",
    "thislk.head()\n",
    "\n",
    "for i in range(np.min((len(thislk),10))):\n",
    "    fn = thislk.file_name.iloc[i]\n",
    "    print(fn)\n",
    "    lk = dictobj(read_melt_lake_h5(fn))\n",
    "    qual_sum = get_quality_summary(lk.detection_quality, lk.lake_quality)\n",
    "    print('quality summary: %.10f | detection_quality: %.7f | lake_quality: %.5f' % (qual_sum, lk.detection_quality, lk.lake_quality))\n",
    "    fig, ax = plt.subplots(figsize=[9,5])\n",
    "    ax.scatter(lk.photon_data.xatc, lk.photon_data.h, s=1, c='k')\n",
    "    isdepth = lk.depth_data.depth>0\n",
    "    ax.scatter(lk.depth_data.xatc[isdepth], lk.depth_data.h_fit_bed[isdepth], s=4, color='r', alpha=lk.depth_data.conf[isdepth])\n",
    "    bed = lk.depth_data.h_fit_bed\n",
    "    bed[~isdepth] = np.nan\n",
    "    ax.plot(lk.depth_data.xatc, lk.depth_data.h_fit_bed, color='gray', lw=0.5)\n",
    "    surf = np.ones_like(lk.depth_data.xatc) * lk.surface_elevation\n",
    "    surf[~isdepth] = np.nan\n",
    "    ax.plot(lk.depth_data.xatc, surf, color='C0', lw=0.8)\n",
    "    rng = lk.surface_elevation - np.min(lk.depth_data.h_fit_bed[isdepth])\n",
    "    ax.set_ylim((lk.depth_data.h_fit_bed[isdepth].min()-rng, lk.surface_elevation+rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e79bc-5ca9-4a5b-bfdd-b3ca4b69d831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ed3b5-2fce-4df0-8f09-731e77f524d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.8f, %.8f' % (lk.lat, lk.lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ff6a5-8766-4637-89c0-7e3ec542fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df_lakes[df_lakes.quality_summary > 0.2]\n",
    "dfd = dfd[dfd.max_depth < 100]\n",
    "dfd = dfd.sort_values(by='max_depth', ascending=False)\n",
    "dfd[:20]\n",
    "for i in range(20):\n",
    "    x = dfd.iloc[i]\n",
    "    print(i, x.max_depth, x.quality_summary)\n",
    "    print(x.file_name)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c09d0c-59e0-47a4-9e30-c2b0ba6034bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [8]:\n",
    "istart = 0\n",
    "ilength = 10\n",
    "for i in np.arange(istart,istart+ilength):\n",
    "    fn = df_lakes.iloc[i].file_name\n",
    "    lk = dictobj(read_melt_lake_h5(fn))\n",
    "    print(fn)\n",
    "    qual_sum = get_quality_summary(lk.detection_quality, lk.lake_quality)\n",
    "    print('quality summary: %.10f | detection_quality: %.7f | lake_quality: %.5f' % (qual_sum, lk.detection_quality, lk.lake_quality))\n",
    "    fig = lk.plot_lake(set_yl='auto', closefig=True)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33077a-44f9-46c3-acbb-74916076bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4aa559-89a1-4331-a999-0c11e3649281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10,20):\n",
    "    fn = dfd.iloc[i].file_name\n",
    "    lk = dictobj(read_melt_lake_h5(fn))\n",
    "    print(fn)\n",
    "    print('detection_quality: %.2f | lake_quality: %.2f' % (lk.detection_quality, lk.lake_quality))\n",
    "    fig = lk.plot_lake(set_yl='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29361689-7dcb-43c5-9c23-48090367090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4].file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b8735-fc12-41a1-9863-ba05cdbdc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ k:v for (k,v) in zip(props_to_get, datalist)}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24f8e7-9795-49fa-bfda-67388724b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk.length_water_surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e935e36-3c90-4ac0-9547-b84bd49e5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_number\n",
    "beam_strength\n",
    "cycle_number\n",
    "granule_id\n",
    "gtx\n",
    "lake_id\n",
    "lat\n",
    "lat_max\n",
    "lat_min\n",
    "lat_str\n",
    "length_extent\n",
    "length_water_surfaces\n",
    "n_photons_where_water\n",
    "lon\n",
    "lon_max\n",
    "lon_min\n",
    "lon_str\n",
    "max_depth\n",
    "polygon_name\n",
    "rgt\n",
    "sc_orient\n",
    "surface_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c910a1-9baf-40ad-bdca-ec8a43717fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2217afd-0e4e-44fe-b412-505d15770f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk.date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298a4ea-3baf-470e-a737-35f19ccd9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk.figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34feaf42-d992-4028-b7df-fc08a8b8dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# totally off, got the surface elevation of the lake wrong...\n",
    "fn = 'lake_07645_AIS_2020-21_simplified_ANT_1000_Peninsula_I-Ipp_ATL03_20201125070417_09480912_006_01_gt2l_0000.h5'\n",
    "# misclassified calving front\n",
    "fn = 'lake_08873_AIS_2018-19_simplified_ANT_1000_West_F-G_ATL03_20190126065316_04400210_006_02_gt2r_0000.h5'\n",
    "# some afterpulse misclassification it seems\n",
    "fn = 'lake_08878_GrIS_2020_simplified_GRE_2000_NE_ATL03_20200831210304_10310805_006_01_gt2r_0006.h5'\n",
    "# finally a good lake\n",
    "fn = 'lake_09090_GrIS_2019_simplified_GRE_2000_CW_ATL03_20190814035453_07190403_006_02_gt3l_0013.h5'\n",
    "# awesome lake!!\n",
    "fn = 'lake_09175_GrIS_2021_simplified_GRE_2000_NE_ATL03_20210720053125_04061205_006_01_gt3r_0042.h5'\n",
    "fn = 'lake_09216_GrIS_2019_simplified_GRE_2000_CW_ATL03_20190818034635_07800403_006_02_gt3l_0015.h5'\n",
    "fn = 'lake_09233_GrIS_2019_simplified_GRE_2000_CW_ATL03_20190810040312_06580403_006_02_gt3l_0011.h5'\n",
    "fn = 'lake_09356_GrIS_2019_simplified_GRE_2000_CW_ATL03_20190716051841_02770403_006_02_gt1l_0000.h5'\n",
    "# insane deadtime returns (up to the 6th or even 7th)\n",
    "fn = 'lake_09241_GrIS_2019_simplified_GRE_2000_NW_ATL03_20190813042032_07040403_006_02_gt2r_0011.h5'\n",
    "# weird data gap? otherwise great lake\n",
    "fn = 'lake_09349_GrIS_2019_simplified_GRE_2000_CW_ATL03_20190810040312_06580403_006_02_gt3r_0014.h5'\n",
    "\n",
    "path = base_dir + out_data_dir + fn\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307b39f-3e23-45b6-9f93-940a30a901e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk = dictobj(read_melt_lake_h5(path))\n",
    "print('detection_quality: %.2f | lake_quality: %.2f' % (lk.detection_quality, lk.lake_quality))\n",
    "fig = lk.plot_lake(set_yl='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11993bdd-1505-4f80-b649-dc1cba79af44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045cd9b-0a00-4283-b1ef-9aefe5bddb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_hdf5(self, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        comp=\"gzip\"\n",
    "        dpdat = f.create_group('depth_data')\n",
    "        dpdat.create_dataset('lon', data=self.depth_data.lon, compression=comp)\n",
    "        dpdat.create_dataset('lat', data=self.depth_data.lat, compression=comp)\n",
    "        dpdat.create_dataset('xatc', data=self.depth_data.xatc, compression=comp)\n",
    "        dpdat.create_dataset('depth', data=self.depth_data.depth, compression=comp)\n",
    "        dpdat.create_dataset('conf', data=self.depth_data.conf, compression=comp)\n",
    "        dpdat.create_dataset('h_fit_surf', data=self.depth_data.h_fit_surf, compression=comp)\n",
    "        dpdat.create_dataset('h_fit_bed', data=self.depth_data.h_fit_bed, compression=comp)\n",
    "        dpdat.create_dataset('std_surf', data=self.depth_data.std_surf, compression=comp)\n",
    "        dpdat.create_dataset('std_bed', data=self.depth_data.std_bed, compression=comp)\n",
    "        \n",
    "        phdat = f.create_group('photon_data')\n",
    "        phdat.create_dataset('lon', data=self.photon_data.lon, compression=comp)\n",
    "        phdat.create_dataset('lat', data=self.photon_data.lat, compression=comp)\n",
    "        phdat.create_dataset('xatc', data=self.photon_data.xatc, compression=comp)\n",
    "        phdat.create_dataset('h', data=self.photon_data.h, compression=comp)\n",
    "        phdat.create_dataset('geoid', data=self.photon_data.geoid, compression=comp)\n",
    "        phdat.create_dataset('snr', data=self.photon_data.snr, compression=comp)\n",
    "        phdat.create_dataset('sat_ratio', data=self.photon_data.sat_ratio, compression=comp)\n",
    "        phdat.create_dataset('sat_elev', data=self.photon_data.sat_elev, compression=comp)\n",
    "        phdat.create_dataset('prob_afterpulse', data=self.photon_data.prob_afterpulse, compression=comp)\n",
    "        phdat.create_dataset('mframe', data=self.photon_data.mframe, compression=comp)\n",
    "        phdat.create_dataset('ph_id_pulse', data=self.photon_data.ph_id_pulse, compression=comp)\n",
    "        phdat.create_dataset('prob_surf', data=self.photon_data.prob_surf, compression=comp)\n",
    "        phdat.create_dataset('prob_bed', data=self.photon_data.prob_bed, compression=comp)\n",
    "        phdat.create_dataset('is_afterpulse', data=self.photon_data.is_afterpulse, compression=comp)\n",
    "\n",
    "        mfdat = f.create_group('mframe_data')\n",
    "        mfdat.create_dataset('mframe', data=self.mframe_data.index, compression=comp)\n",
    "        mfdat.create_dataset('lon', data=self.mframe_data.lon, compression=comp)\n",
    "        mfdat.create_dataset('lat', data=self.mframe_data.lat, compression=comp)\n",
    "        mfdat.create_dataset('xatc', data=self.mframe_data.xatc, compression=comp)\n",
    "        mfdat.create_dataset('dt', data=self.mframe_data.dt, compression=comp)\n",
    "        mfdat.create_dataset('xatc_min', data=self.mframe_data.xatc_min, compression=comp)\n",
    "        mfdat.create_dataset('xatc_max', data=self.mframe_data.xatc_max, compression=comp)\n",
    "        mfdat.create_dataset('n_phot', data=self.mframe_data.n_phot, compression=comp)\n",
    "        mfdat.create_dataset('peak', data=self.mframe_data.peak, compression=comp)\n",
    "        mfdat.create_dataset('is_flat', data=self.mframe_data.is_flat, compression=comp)\n",
    "        mfdat.create_dataset('lake_qual_pass', data=self.mframe_data.lake_qual_pass, compression=comp)\n",
    "        mfdat.create_dataset('quality_summary', data=self.mframe_data.quality_summary, compression=comp)\n",
    "        mfdat.create_dataset('snr_surf', data=self.mframe_data.snr_surf, compression=comp)\n",
    "        mfdat.create_dataset('snr_upper', data=self.mframe_data.snr_upper, compression=comp)\n",
    "        mfdat.create_dataset('snr_lower', data=self.mframe_data.snr_lower, compression=comp)\n",
    "        mfdat.create_dataset('snr_allabove', data=self.mframe_data.snr_allabove, compression=comp)\n",
    "        mfdat.create_dataset('ratio_2nd_returns', data=self.mframe_data.ratio_2nd_returns, compression=comp)\n",
    "        mfdat.create_dataset('quality_secondreturns', data=self.mframe_data.quality_secondreturns, compression=comp)\n",
    "        mfdat.create_dataset('alignment_penalty', data=self.mframe_data.alignment_penalty, compression=comp)\n",
    "        mfdat.create_dataset('range_penalty', data=self.mframe_data.range_penalty, compression=comp)\n",
    "        mfdat.create_dataset('length_penalty', data=self.mframe_data.length_penalty, compression=comp)\n",
    "\n",
    "        scnds = f.create_group('detection_2nd_returns')\n",
    "        scnds.create_dataset('h', data=np.array(self.detection_2nd_returns['h']), compression=comp)\n",
    "        scnds.create_dataset('xatc', data=np.array(self.detection_2nd_returns['xatc']), compression=comp)\n",
    "        scnds.create_dataset('prom', data=np.array(self.detection_2nd_returns['prom']), compression=comp)\n",
    "\n",
    "        dqinf = f.create_group('detection_quality_info')\n",
    "        for k in self.detection_quality_info.keys():\n",
    "            dqinf.create_dataset(k, data=np.array(self.detection_quality_info[k]))\n",
    "\n",
    "        props = f.create_group('properties')\n",
    "        props.create_dataset('lake_id', data=self.lake_id)\n",
    "        props.create_dataset('lake_quality', data=self.lake_quality)\n",
    "        props.create_dataset('max_depth', data=self.max_depth)\n",
    "        props.create_dataset('mframe_start', data=self.mframe_start)\n",
    "        props.create_dataset('mframe_end', data=self.mframe_end)\n",
    "        props.create_dataset('main_peak', data=self.main_peak)\n",
    "        props.create_dataset('n_subsegs_per_mframe', data=self.n_subsegs_per_mframe)\n",
    "        props.create_dataset('len_subsegs', data=self.len_subsegs)\n",
    "        props.create_dataset('surface_extent_detection', data=np.array([x for y in self.surface_extent_detection for x in y]))\n",
    "        props.create_dataset('lat_surface_extent_detection', data=np.array([x for y in self.lat_surface_extent_detection for x in y]))\n",
    "        props.create_dataset('length_extent', data=self.length_extent)\n",
    "        props.create_dataset('full_lat_extent_detection', data=self.full_lat_extent_detection)\n",
    "        props.create_dataset('lat_min', data=self.lat_min)\n",
    "        props.create_dataset('lat_max', data=self.lat_max)\n",
    "        props.create_dataset('lat', data=self.lat)\n",
    "        props.create_dataset('lat_str', data=self.lat_str.replace('°',''))\n",
    "        props.create_dataset('lon_min', data=self.lon_min)\n",
    "        props.create_dataset('lon_max', data=self.lon_max)\n",
    "        props.create_dataset('lon', data=self.lon)\n",
    "        props.create_dataset('lon_str', data=self.lon_str.replace('°',''))\n",
    "        props.create_dataset('beam_number', data=self.beam_number)\n",
    "        props.create_dataset('beam_strength', data=self.beam_strength)\n",
    "        props.create_dataset('cycle_number', data=self.cycle_number)\n",
    "        props.create_dataset('sc_orient', data=self.sc_orient)\n",
    "        props.create_dataset('dead_time', data=self.dead_time)\n",
    "        props.create_dataset('dead_time_meters', data=self.dead_time_meters)\n",
    "        props.create_dataset('polygon_filename', data=self.polygon_filename)\n",
    "        props.create_dataset('polygon_name', data=self.polygon_name)\n",
    "        props.create_dataset('length_water_surfaces', data=self.length_water_surfaces)\n",
    "        props.create_dataset('n_photons_where_water', data=self.n_photons_where_water)\n",
    "        props.create_dataset('detection_quality', data=self.detection_quality)\n",
    "        props.create_dataset('surface_elevation', data=self.surface_elevation)\n",
    "        props.create_dataset('oaurl', data=self.oaurl)\n",
    "        props.create_dataset('gtx', data=self.gtx)\n",
    "        props.create_dataset('rgt', data=self.rgt)\n",
    "        props.create_dataset('granule_id', data=self.granule_id)\n",
    "        props.create_dataset('melt_season', data=self.melt_season)\n",
    "        props.create_dataset('ice_sheet', data=self.ice_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994d518-8e23-4d9d-a922-860ea78461b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeicelakes-env",
   "language": "python",
   "name": "eeicelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
